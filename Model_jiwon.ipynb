{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 전처리 반영 코드"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data 불러오기\n",
    "- import version 설정(터미널에서 지정)\n",
    "    * pip install scikit-learn==1.1\n",
    "    * pip install imbalanced-learn==0.9\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## data 확인\n",
    "데이터 출처: Kaggle\n",
    "https://www.kaggle.com/code/joshuaswords/awesome-hr-data-visualization-prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors\n",
    "import matplotlib.ticker as mtick\n",
    "\n",
    "# To ensure text placement and advanced grid layout\n",
    "from matplotlib.gridspec import GridSpec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aug_test에는 target값이 없어서 aug_train만 사용할 예정\n",
    "## 그리고 aug_train에서 train, validation, test set을 6:2:2로 나눌 예정\n",
    "aug_train = pd.read_csv('data/aug_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 결측치 확인\n",
    "결측치가 꽤나 많은데 다 살리는 방향으로 갈 거다.\n",
    "- enrollee_id                  0\n",
    "- city                         0\n",
    "- city_development_index       0\n",
    "- gender                    4508\n",
    "- relevent_experience          0\n",
    "- enrolled_university        386\n",
    "- education_level            460\n",
    "- major_discipline          2813\n",
    "- experience                  65\n",
    "- company_size              5938\n",
    "- company_type              6140\n",
    "- last_new_job               423\n",
    "- training_hours               0\n",
    "- target                       0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for nulls : 결측치 비율 확인\n",
    "aug_train.isna().sum()/len(aug_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결측치가 아예 없었던 데이터 10203개\n",
    "# aug_train[aug_train.isnull().any(axis=1)]\n",
    "aug_train.isna().sum(axis=1).astype(bool).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 실수형, 문자형 변수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 실수형, 문자형 변수\n",
    "var_num = aug_train.select_dtypes(include=['float', 'int']).columns\n",
    "var_obj = aug_train.select_dtypes(include=['object']).columns\n",
    "\n",
    "print('features with dtype float : ', len(var_num),'개')\n",
    "print(var_num)\n",
    "print('features with dtype object : ', len(var_obj),'개')\n",
    "print(var_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_num = aug_train[['enrollee_id', 'city_development_index', 'training_hours', 'target']]\n",
    "group_obj = aug_train[['city', 'gender', 'relevent_experience', 'enrolled_university', 'education_level', 'major_discipline', 'experience', 'company_size',\n",
    "                       'company_type', 'last_new_job']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 실수형 변수\n",
    "- 실수형 변수들은 결측치가 없다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_num.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 범주형 자료 : categorical data 정리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_obj.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### city\n",
    "- city별로 one unique한 city_development_index 값을 가지기 때문에 city는 제거한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'city' and calculate the number of unique 'city_development_index' values\n",
    "unique_development_index_counts = aug_train.groupby('city')['city_development_index'].nunique().reset_index()\n",
    "\n",
    "# Check if any city has more than one unique city_development_index value\n",
    "cities_with_multiple_development_indices = unique_development_index_counts[unique_development_index_counts['city_development_index'] > 1]\n",
    "\n",
    "# Display the results\n",
    "print(f\"Cities with more than one unique 'city_development_index':\\n{cities_with_multiple_development_indices}\")\n",
    "\n",
    "# Display the unique counts for all cities\n",
    "print(f\"\\nUnique 'city_development_index' counts for all cities:\\n{unique_development_index_counts}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_train['city'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_train.drop(columns=['city'],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### last_new_job 변수\n",
    "결측치 처리 방법\n",
    "1. (aug_train['company_type'].isna()) & (aug_train['company_size'].isna()) 조건에 맞는 행의 'last_new_job'을 'never'로 채움\n",
    "    - never의 의미는 한 번도 직장을 구해보지 않았다는 의미다.\n",
    "2. 이후의 남은 결측치는 NaN       230 => Unknown으로 채울 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_train[(aug_train['company_type'].isna())&(aug_train['company_size'].isna())]['last_new_job'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필터링 조건에 맞는 행의 'last_new_job'을 'never'로 채움\n",
    "mask = (aug_train['company_type'].isna()) & (aug_train['company_size'].isna())\n",
    "aug_train.loc[mask, 'last_new_job'] = aug_train.loc[mask, 'last_new_job'].fillna('never')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_train[(aug_train['company_type'].isna())&(aug_train['company_size'].isna())]['last_new_job'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_train['last_new_job'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- NaN 230개는 나중에 experience 결측치 처리할 때 다시 보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_train['last_new_job'].fillna('Unknown',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 보기 좋게만 만들기\n",
    "aug_train['last_new_job'] = aug_train['last_new_job'].apply(lambda x: 'Never' if x == 'never' else x) #just reads nicer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_train['last_new_job'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### experience 변수\n",
    "1. 변수 value 값 처리\n",
    "    - x: '0' if x == '<1' else x\n",
    "        - 대신, 'experience' 열을 기준으로 'experience_under_1' 열 생성\n",
    "    - x: '0' if x == '>20' else x\n",
    "        - 대신, 'experience' 열을 기준으로 'experience_over_20' 열 생성\n",
    "2. 결측치 : NaN      65개 발생\n",
    "    - 'experience' 열을 기준으로 x가 결측치면 1, 아니면 0으로 'experience_Unknown' 열 생성\n",
    "    - 'experience' 열에서 결측치는 '0'으로 대체\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_train['experience'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'experience' 열을 기준으로 'experience_under_1' 열 생성\n",
    "aug_train['experience_under_1'] = aug_train['experience'].apply(lambda x: 1 if x == '<1' else 0)\n",
    "\n",
    "# 'experience' 열을 기준으로 'experience_over_20' 열 생성\n",
    "aug_train['experience_over_20'] = aug_train['experience'].apply(lambda x: 1 if x == '>20' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_train[aug_train['experience']=='0']['experience_over_20'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_train[aug_train['experience']=='0']['experience_under_1'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_train['experience_over_20'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_train['experience_under_1'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'experience' 열에서 '<1'을 '0'으로 대체\n",
    "aug_train['experience'] = aug_train['experience'].apply(lambda x: '0' if x == '<1' else x)\n",
    "\n",
    "# 'experience' 열에서 '>20'을 '0'으로 대체\n",
    "aug_train['experience'] = aug_train['experience'].apply(lambda x: '0' if x == '>20' else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'experience' 열을 기준으로 x가 결측치면 1, 아니면 0으로 'experience_Unknown' 열 생성\n",
    "aug_train['experience_Unknown'] = aug_train['experience'].apply(lambda x: 1 if pd.isna(x) else 0)\n",
    "\n",
    "# 'experience' 열에서 결측치는 '0'으로 대체\n",
    "aug_train['experience'] = aug_train['experience'].fillna('0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_train['experience'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_train['experience_Unknown'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### company 변수\n",
    "###### company_size 변수 \n",
    "- 결측치 :NaN          5938 개,\n",
    "###### company_type 변수 \n",
    "- 결측치 :NaN                    6140개,\n",
    "1. (1711개) company 변수 둘 다 결측치가 있고, last_new_job ='never'인 경우에는 company_type, company_size 둘 다 'Unemployed'\n",
    "    - last_new_job : 이전 직업과 현재 직업의 몇 년 차이\n",
    "    - 앞으로 다닐 회사가 아니라, last_new_job가 'never'인 즉, 이전회사가 없었던 얘들은 unemployed 된 애들이라고 생각함\n",
    "\n",
    "2. 나머지는 'Unknown'으로 만들기\n",
    "- aug_train['company_size'].fillna('Unknown',inplace=True)\n",
    "- aug_train['company_type'].fillna('Unknown',inplace=True)\n",
    "    - 결측치 겹치는 개 : 5360개 -> 얘네는 last_new_job과는 공통점이 없다.\n",
    "    - company 정보가 현재 회사를 다니고 있고, 회사를 다니는 중에 교육을 받는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_train['company_size'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_train['company_type'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_train[(aug_train['last_new_job']=='Never')&(aug_train['company_size'].isna())]['company_type'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_train[(aug_train['last_new_job']=='Never')&(aug_train['company_type'].isna())]['company_size'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Last_new_job이 never이면서 company size가 결측인게 없음\n",
    "* 마찬가지로 last_new_job이 never이면서 company_type이 결측인게 없음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필터링 조건 정의\n",
    "mask = (aug_train['last_new_job']=='Never')&(aug_train['company_type'].isna())\n",
    "\n",
    "# 'company_size'의 NaN 값을 'Unemployed'으로 채움\n",
    "aug_train.loc[mask, 'company_size'] = aug_train.loc[mask, 'company_size'].fillna('Unemployed')\n",
    "\n",
    "# 필터링 조건 정의\n",
    "mask = (aug_train['last_new_job']=='Never')&(aug_train['company_size']=='Unemployed')\n",
    "# 'company_type'의 NaN 값을 'Unemployed'으로 채움\n",
    "aug_train.loc[mask, 'company_type'] = aug_train.loc[mask, 'company_type'].fillna('Unemployed')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(aug_train['company_type'].value_counts(dropna=False))\n",
    "print(aug_train['company_size'].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# company_size 0으로 처리하는 걸로 바꿈.\n",
    "aug_train['company_size'].fillna('0',inplace=True)\n",
    "aug_train['company_type'].fillna('Unknown',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 보기 좋게만 만들기\n",
    "aug_train['company_size'] = aug_train['company_size'].apply(lambda x: '10-49' if x == '10/49' else x) #diff replacement method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(aug_train['company_type'].value_counts(dropna=False))\n",
    "print(aug_train['company_size'].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Gender variable\n",
    "- 결측치와 Other이랑 합친다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_train['gender'].fillna('Not provided',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_train['gender'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필터링 조건 수정: 'Not provided' 또는 'Other'\n",
    "aug_train_gender = aug_train[(aug_train['gender'] == 'Not provided') | (aug_train['gender'] == 'Other')]\n",
    "aug_train_gender['gender'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_num = aug_train[['enrollee_id', 'city_development_index', 'training_hours', 'target']]\n",
    "group_obj = aug_train[['gender', 'relevent_experience', 'enrolled_university', 'education_level', 'major_discipline', 'experience', 'company_size',\n",
    "                       'company_type', 'last_new_job']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### genderd의 'other'와 'not provided'의 분포 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('수치형 데이터 시각화')\n",
    "# 수치형 데이터 시각화\n",
    "for column in group_num:\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.histplot(data=aug_train_gender, x=column, hue='gender', multiple='dodge', palette='Set2')\n",
    "    plt.title(f'{column} Distribution by Gender')\n",
    "    plt.xlabel(column)\n",
    "    plt.ylabel('Count')\n",
    "    plt.legend(title='Gender', labels=['Other', 'Not provided'])\n",
    "    plt.show()\n",
    "\n",
    "print('범주형 데이터 시각화')\n",
    "# 범주형 데이터 시각화\n",
    "for column in group_obj:\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.countplot(data=aug_train_gender, x=column, hue='gender', palette='Set2')\n",
    "    plt.title(f'{column} Distribution by Gender')\n",
    "    plt.xlabel(column)\n",
    "    plt.ylabel('Count')\n",
    "    plt.legend(title='Gender', labels=['Not provided','Other'])\n",
    "    plt.xticks(rotation=45)  # 범주형 데이터의 레이블이 길 경우 각도를 조정\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'gender'가 'Not provided'인 행의 'gender'를 'Other'로 변경\n",
    "aug_train.loc[aug_train['gender'] == 'Not provided', 'gender'] = 'Other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_train['gender'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Major variable\n",
    "- 결측치와 Other이랑 합치려 했으나... education_level 에 분포의 차이가 있음\n",
    "- 결측치는 'Unknown'으로, 'No Major'와 'Other' 도 각각 따로 가지고 가기로 결정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_train['major_discipline'].fillna('Unknown',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필터링 조건 수정: 'Unknown' 또는 'Other'\n",
    "aug_train_major_discipline = aug_train[(aug_train['major_discipline'] == 'Unknown') | (aug_train['major_discipline'] == 'Other')]\n",
    "aug_train_major_discipline['major_discipline'].value_counts(dropna= False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### major dicipline의 'other'와 'unknown'의 분포 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_num = aug_train[['enrollee_id', 'city_development_index', 'training_hours', 'target']]\n",
    "group_obj = aug_train[['gender', 'relevent_experience', 'enrolled_university', 'education_level', 'major_discipline', 'experience', 'company_size',\n",
    "                       'company_type', 'last_new_job']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('수치형 데이터 시각화')\n",
    "# 수치형 데이터 시각화\n",
    "for column in group_num:\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.histplot(data=aug_train_major_discipline, x=column, hue='major_discipline', multiple='dodge', palette='Set2')\n",
    "    plt.title(f'{column} Distribution by major_discipline')\n",
    "    plt.xlabel(column)\n",
    "    plt.ylabel('Count')\n",
    "    plt.legend(title='Major Discipline', labels=['Other','Unknown'])\n",
    "    plt.show()\n",
    "\n",
    "print('범주형 데이터 시각화')\n",
    "# 범주형 데이터 시각화\n",
    "for column in group_obj:\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.histplot(data=aug_train_major_discipline, x=column, hue='major_discipline',multiple='dodge', palette='Set2')\n",
    "    plt.title(f'{column} Distribution by major_discipline')\n",
    "    plt.xlabel(column)\n",
    "    plt.ylabel('Count')\n",
    "    plt.legend(title='Major Discipline', labels=['Other', 'Unknown'])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_train['major_discipline'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### enrolled_university\n",
    "- 결측치 : NaN                   386개(0.020148, 비율) => 'Unknown'으로"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_train['enrolled_university'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_train[aug_train['education_level'].isnull()]['enrolled_university'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 보기 좋게만 만들기\n",
    "aug_train['enrolled_university'][aug_train['enrolled_university'] == 'no_enrollment'] = 'No Enrollment' #just reads nicer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_train['enrolled_university'].fillna(\"Unknown\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_train['enrolled_university'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### education_level\n",
    "- 결측치 460개,           0.024011 ==> Unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_train['education_level'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_train[aug_train['enrolled_university']=='Unknown']['education_level'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_train.loc[aug_train['enrolled_university']=='no_enrollment','education_level'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_train[aug_train['enrolled_university']=='No Enrollment']['education_level'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_train['education_level'].fillna(\"Unknown\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_train['education_level'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 결측치 있는 행 확인\n",
    "- 결측치를 다 Unknown 살리고,\n",
    "- Unknown 나름의 의미가 있다고 본다.\n",
    "\n",
    "- 결측치로 인해서 데이터 행을 삭제하지 않고, 다 살려서 진행했다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_train.isna().sum()/len(aug_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 정수형으로 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(aug_train.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting objects to integers\n",
    "aug_train['experience'] = aug_train['experience'].astype(str).astype(int)\n",
    "aug_train['target'] = aug_train['target'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(aug_train.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 변수 value 임의의 순서로 지정하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 열의 고유 값 출력\n",
    "for column in aug_train.columns:\n",
    "    unique_values = aug_train[column].unique()\n",
    "    print(f\"Column '{column}' unique values: {unique_values}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Orders\n",
    "ed_order = ['Primary School','High School','Graduate','Masters','Phd','Unknown']\n",
    "enroll_order = ['No Enrollment','Part time course','Full time course','Unknown']\n",
    "disc_order = ['STEM','Humanities','Business Degree','Arts','Other','No Major','Unknown']\n",
    "exp_yrs_order = ['0','<1','1','2','3','4','5','6','7','8','9','10','11','12','13','14','15','16','17','18','19','20','>20']\n",
    "exp_yrs_order_2 = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]\n",
    "size_order = ['0','<10', '10-49', '50-99', '100-500', '500-999', '1000-4999', '5000-9999', '10000+','Unemployed']\n",
    "job_order = ['Never', '1', '2', '3', '4', '>4','Unknown']\n",
    "exp_order =['No relevant experience','Has relevant experience']\n",
    "gender_order = ['Male','Female','Other']\n",
    "company_order = ['Pvt Ltd','Funded Startup','Public Sector','Early Stage Startup','NGO','Other','Unknown','Unemployed']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 코드는 aug_train 데이터프레임을 사용하여 교육 수준(education_level)과 마지막 직업 변경 시기(last_new_job)에 따른 직업 경험(experience)을 분석하고, 직업을 찾는 사람과 그렇지 않은 사람의 비율을 비교하기 위해 데이터를 정규화하고 요약하는 과정을 포함하고 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 각 변수별 유니크 값 확인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 14개 -> 총 33개 칼럼!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ------지선 -> 윤지 확인-----(지선 전처리에서 추가 코드된 부분이랑 수정된 부분 있어요!)----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 전처리 다하고 EDA 시작하기 전에, train, val, test 셋 분리하는 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = aug_train.dropna().drop(columns=['target']).values\n",
    "y = aug_train.dropna()['target'].values\n",
    "\n",
    "# train, val, test 셋 분리하는 코드\n",
    "# 데이터셋 분할: Train/Validation + Test\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=80)\n",
    "# 데이터셋 분할: Train + Validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.2, random_state=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 열 이름 설정\n",
    "feature_columns = aug_train.drop(columns=['target']).columns\n",
    "target_column = ['target']\n",
    "\n",
    "# numpy 배열을 DataFrame으로 변환\n",
    "X_train_df = pd.DataFrame(X_train_val, columns=feature_columns)\n",
    "X_val_df = pd.DataFrame(X_val, columns=feature_columns)\n",
    "X_test_df = pd.DataFrame(X_test, columns=feature_columns)\n",
    "y_train_df = pd.DataFrame(y_train_val, columns=target_column)\n",
    "y_val_df = pd.DataFrame(y_val, columns=target_column)\n",
    "y_test_df = pd.DataFrame(y_test, columns=target_column)\n",
    "\n",
    "# Ensure 'train_df' and 'test_df' are created correctly by concatenating X and y\n",
    "# train_df : for visuals\n",
    "train_df = pd.concat([X_train_df, y_train_df], axis=1)\n",
    "test_df = pd.concat([X_val_df, y_val_df], axis=1)\n",
    "new_data_df = pd.concat([X_test_df, y_test_df], axis=1)\n",
    "\n",
    "# 출력 확인 : for visual\n",
    "print(train_df.info())\n",
    "print(test_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = X_train, X_val, y_train, y_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EDA 전, unique한 값 확인\n",
    "- 일단 모든 변수 object형이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#relevent_experience\n",
    "aug_train['relevent_experience'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "aug_train['enrolled_university'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "aug_train['education_level'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "aug_train['major_discipline'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "aug_train['experience'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_train['company_size'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_train['company_type'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_train['last_new_job'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_train['last_new_job'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_train['training_hours'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 기존 EDA 정리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.palplot(['#002d1d','#0e4f66','gray','#fbfbfb'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=train_df.groupby(['target'])['target'].count()\n",
    "y=len(train_df)\n",
    "r=((x/y)).round(2)\n",
    "ratio = pd.DataFrame(r).T\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1,1,figsize=(6.5, 2),dpi=150)\n",
    "background_color = \"#fbfbfb\"\n",
    "fig.patch.set_facecolor(background_color)\n",
    "ax.set_facecolor(background_color) \n",
    "\n",
    "ax.barh(ratio.index, ratio[1.0], color='#0e4f66', alpha=0.9, ec=background_color, label='Job-Seeker')\n",
    "ax.barh(ratio.index, ratio[0.0], left=ratio[1.0], color='gray', alpha=0.9,ec=background_color, label='Non Job-Seeker')\n",
    "\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "ax.legend().set_visible(False)\n",
    "for s in ['top', 'left', 'right', 'bottom']:\n",
    "    ax.spines[s].set_visible(False)\n",
    "    \n",
    "for i in ratio.index:\n",
    "    ax.annotate(f\"{int(ratio[1.0][i]*100)}%\", xy=(ratio[1.0][i]/2, i),va = 'center', ha='center',fontsize=32, fontweight='light', fontfamily='serif',color='white')\n",
    "    ax.annotate(\"Job-Seeker\", xy=(ratio[1.0][i]/2, -0.25),va = 'center', ha='center',fontsize=12, fontweight='light', fontfamily='serif',color='white')\n",
    "    \n",
    "    \n",
    "for i in ratio.index:\n",
    "    ax.annotate(f\"{int(ratio[0.0][i]*100)}%\", xy=(ratio[1.0][i]+ratio[0.0][i]/2, i),va = 'center', ha='center',fontsize=32, fontweight='light', fontfamily='serif',color='white')\n",
    "    ax.annotate(\"Non Job-Seeker\", xy=(ratio[1.0][i]+ratio[0.0][i]/2, -0.25),va = 'center', ha='center',fontsize=12, fontweight='light', fontfamily='serif',color='white')\n",
    "\n",
    "\n",
    "fig.text(0.125,1.1,'How many are looking for a new role?', fontfamily='serif',fontsize=15, fontweight='bold')\n",
    "fig.text(0.125,0.915,'We see an imbalanced dataset;\\nmost people are not job-seeking',fontfamily='serif',fontsize=12)  \n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Columns in train_df:\", train_df.columns)\n",
    "print(\"Columns in test_df:\", test_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "background_color = \"#fbfbfb\"\n",
    "\n",
    "fig = plt.figure(figsize=(22,15),dpi=150)\n",
    "fig.patch.set_facecolor(background_color)\n",
    "gs = fig.add_gridspec(3, 3)\n",
    "gs.update(wspace=0.35, hspace=0.27)\n",
    "ax0 = fig.add_subplot(gs[0, 0])\n",
    "ax1 = fig.add_subplot(gs[0, 1])\n",
    "ax2 = fig.add_subplot(gs[0, 2])\n",
    "ax3 = fig.add_subplot(gs[1, 0])\n",
    "ax4 = fig.add_subplot(gs[1, 1])\n",
    "ax5 = fig.add_subplot(gs[1, 2])\n",
    "ax6 = fig.add_subplot(gs[2, 0])\n",
    "ax7 = fig.add_subplot(gs[2, 1])\n",
    "ax8 = fig.add_subplot(gs[2, 2])\n",
    "\n",
    "\n",
    "# Ax0 - EDUCATION LEVEL\n",
    "train = pd.DataFrame(train_df[\"education_level\"].value_counts())\n",
    "train[\"Percentage\"] = train[\"education_level\"].apply(lambda x: x/sum(train[\"education_level\"])*100)\n",
    "train = train.sort_index()\n",
    "\n",
    "test = pd.DataFrame(test_df[\"education_level\"].value_counts())\n",
    "test[\"Percentage\"] = test[\"education_level\"].apply(lambda x: x/sum(test[\"education_level\"])*100)\n",
    "test = test.sort_index()\n",
    "\n",
    "ax0.bar(np.arange(len(train.index)), height=train[\"Percentage\"], zorder=3, color=\"gray\", width=0.05)\n",
    "ax0.scatter(np.arange(len(train.index)), train[\"Percentage\"], zorder=3,s=200, color=\"gray\")\n",
    "ax0.bar(np.arange(len(test.index))+0.4, height=test[\"Percentage\"], zorder=3, color=\"#0e4f66\", width=0.05)\n",
    "ax0.scatter(np.arange(len(test.index))+0.4, test[\"Percentage\"], zorder=3,s=200, color=\"#0e4f66\")\n",
    "ax0.text(-0.5, 68.5, 'Education Level', fontsize=14, fontweight='bold', fontfamily='serif', color=\"#323232\")\n",
    "ax0.yaxis.set_major_formatter(mtick.PercentFormatter())\n",
    "ax0.yaxis.set_major_locator(mtick.MultipleLocator(10))\n",
    "ax0.set_xticks(np.arange(len(train.index))+0.4 / 2)\n",
    "ax0.set_xticklabels(list(train.index),rotation=0)\n",
    "\n",
    "\n",
    "# Ax1 - ENROLLED IN UNIVESITY\n",
    "train = pd.DataFrame(train_df[\"enrolled_university\"].value_counts())\n",
    "train[\"Percentage\"] = train[\"enrolled_university\"].apply(lambda x: x/sum(train[\"enrolled_university\"])*100).loc[enroll_order]\n",
    "test = pd.DataFrame(test_df[\"enrolled_university\"].value_counts())\n",
    "test[\"Percentage\"] = test[\"enrolled_university\"].apply(lambda x: x/sum(test[\"enrolled_university\"])*100).loc[enroll_order]\n",
    "\n",
    "ax1.text(0, 2.5, 'University Enrollment', fontsize=14, fontweight='bold', fontfamily='serif', color=\"#323232\")\n",
    "ax1.barh(train.index, train['Percentage'], color=\"gray\", zorder=3, height=0.6)\n",
    "ax1.barh(test.index, test['Percentage'], color=\"#0e4f66\", zorder=3, height=0.4)\n",
    "ax1.xaxis.set_major_formatter(mtick.PercentFormatter())\n",
    "ax1.xaxis.set_major_locator(mtick.MultipleLocator(10))\n",
    "\n",
    "###\n",
    "# Ax2 - GENDER \n",
    "train = pd.DataFrame(train_df[\"gender\"].value_counts())\n",
    "train[\"Percentage\"] = train[\"gender\"].apply(lambda x: x/sum(train[\"gender\"])*100)\n",
    "test = pd.DataFrame(test_df[\"gender\"].value_counts())\n",
    "test[\"Percentage\"] = test[\"gender\"].apply(lambda x: x/sum(test[\"gender\"])*100)\n",
    "\n",
    "x = np.arange(len(train))\n",
    "ax2.text(-0.6, 76, 'Gender', fontsize=14, fontweight='bold', fontfamily='serif', color=\"#323232\")\n",
    "ax2.grid(color='gray', linestyle=':', axis='y', zorder=0,  dashes=(1,5))\n",
    "ax2.bar(x, height=train[\"Percentage\"], zorder=3, color=\"gray\", width=0.4)\n",
    "ax2.bar(x+0.4, height=test[\"Percentage\"], zorder=3, color=\"#0e4f66\", width=0.4)\n",
    "ax2.set_xticks(x + 0.4 / 2)\n",
    "ax2.set_xticklabels(['Male','Female','Other'])\n",
    "ax2.yaxis.set_major_formatter(mtick.PercentFormatter())\n",
    "ax2.yaxis.set_major_locator(mtick.MultipleLocator(10))\n",
    "for i,j in zip([0, 1], train[\"Percentage\"]):\n",
    "    ax2.annotate(f'{j:0.0f}%',xy=(i, j/2), color='#f6f6f6', horizontalalignment='center', verticalalignment='center')\n",
    "for i,j in zip([0, 1], test[\"Percentage\"]):\n",
    "    ax2.annotate(f'{j:0.0f}%',xy=(i+0.4, j/2), color='#f6f6f6', horizontalalignment='center', verticalalignment='center')\n",
    "    \n",
    "\n",
    "    \n",
    "## Ax 3 - CDI\n",
    "\n",
    "ax3.grid(color='gray', linestyle=':', axis='y', zorder=0,  dashes=(1,5))\n",
    "train = pd.DataFrame(train_df[\"city_development_index\"])\n",
    "test = pd.DataFrame(test_df[\"city_development_index\"])\n",
    "sns.kdeplot(train[\"city_development_index\"], ax=ax3,color=\"gray\", shade=True, label=\"Train\")\n",
    "sns.kdeplot(test[\"city_development_index\"], ax=ax3, color=\"#0e4f66\", shade=True, label=\"Test\")\n",
    "ax3.text(0.29, 13, 'City Development Index', fontsize=14, fontweight='bold', fontfamily='serif', color=\"#323232\")\n",
    "ax3.yaxis.set_major_locator(mtick.MultipleLocator(2))\n",
    "ax3.set_ylabel('')    \n",
    "ax3.set_xlabel('')\n",
    "\n",
    "## AX4 - TITLE\n",
    "\n",
    "ax4.spines[\"bottom\"].set_visible(False)\n",
    "ax4.tick_params(left=False, bottom=False)\n",
    "ax4.set_xticklabels([])\n",
    "ax4.set_yticklabels([])\n",
    "ax4.text(0.5, 0.6, 'How do our\\n\\n datasets compare?', horizontalalignment='center', verticalalignment='center',fontsize=22, fontweight='bold', fontfamily='serif', color=\"#323232\")\n",
    "ax4.text(0.28,0.57,\"Train\", fontweight=\"bold\", fontfamily='serif', fontsize=22, color='gray')\n",
    "ax4.text(0.5,0.57,\"&\", fontweight=\"bold\", fontfamily='serif', fontsize=22, color='#323232')\n",
    "ax4.text(0.58,0.57,\"Test\", fontweight=\"bold\", fontfamily='serif', fontsize=22, color='#0e4f66')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Ax5 - RELEVANT EXPERIENCE\n",
    "train = pd.DataFrame(train_df[\"relevent_experience\"].value_counts())\n",
    "train[\"Percentage\"] = train[\"relevent_experience\"].apply(lambda x: x/sum(train[\"relevent_experience\"])*100)\n",
    "test = pd.DataFrame(test_df[\"relevent_experience\"].value_counts())\n",
    "test[\"Percentage\"] = test[\"relevent_experience\"].apply(lambda x: x/sum(test[\"relevent_experience\"])*100)\n",
    "x = np.arange(len(train))\n",
    "ax5.text(-0.4, 80, 'Relevant Experience', fontsize=14, fontweight='bold', fontfamily='serif', color=\"#323232\")\n",
    "ax5.grid(color='gray', linestyle=':', axis='y', zorder=0,  dashes=(1,5))\n",
    "ax5.bar(x, height=train[\"Percentage\"], zorder=3, color=\"gray\", width=0.4)\n",
    "ax5.bar(x+0.4, height=test[\"Percentage\"], zorder=3, color=\"#0e4f66\", width=0.4)\n",
    "ax5.set_xticks(x + 0.4 / 2)\n",
    "ax5.set_xticklabels(['No relevant experience','Has relevant experience'])\n",
    "ax5.yaxis.set_major_formatter(mtick.PercentFormatter())\n",
    "ax5.yaxis.set_major_locator(mtick.MultipleLocator(10))\n",
    "\n",
    "for i,j in zip([0, 1, 2], train[\"Percentage\"]):\n",
    "    ax5.annotate(f'{j:0.0f}%',xy=(i, j/2), color='#f6f6f6', horizontalalignment='center', verticalalignment='center')\n",
    "for i,j in zip([0, 1, 2], test[\"Percentage\"]):\n",
    "    ax5.annotate(f'{j:0.0f}%',xy=(i+0.4, j/2), color='#f6f6f6', horizontalalignment='center', verticalalignment='center')\n",
    "\n",
    "    \n",
    "    \n",
    "# Ax6 - TRAINING HOURS\n",
    "train = pd.DataFrame(train_df[\"training_hours\"])\n",
    "train[\"TrainTest\"] = \"Train\"\n",
    "test = pd.DataFrame(test_df[\"training_hours\"])\n",
    "test[\"TrainTest\"] = \"Test\"\n",
    "ax6.text(-0.65, 370, 'Training Hours', fontsize=14, fontweight='bold', fontfamily='serif', color=\"#002d1d\")\n",
    "comb_graph_temp_df = pd.concat([train, test], axis=0)\n",
    "sns.boxenplot(ax=ax6, y=\"training_hours\", x=\"TrainTest\", data=comb_graph_temp_df, palette=[\"gray\", \"#0e4f66\"])\n",
    "ax6.set_xlabel(\"\")\n",
    "ax6.set_ylabel(\"\")\n",
    "\n",
    "\n",
    "# Ax7 - EXPERIENCE YRS\n",
    "train = pd.DataFrame(train_df[\"experience\"].value_counts())\n",
    "train[\"Percentage\"] = train[\"experience\"].apply(lambda x: x/sum(train[\"experience\"])*100)\n",
    "train = train.sort_index()\n",
    "test = pd.DataFrame(test_df[\"experience\"].value_counts())\n",
    "test[\"Percentage\"] = round(test[\"experience\"].apply(lambda x: x/sum(test[\"experience\"])*100),).astype(int)\n",
    "test = test.sort_index()\n",
    "ax7.grid(color='gray', linestyle=':', axis='y', zorder=0,  dashes=(1,5))\n",
    "ax7.plot(train.index, train[\"Percentage\"], zorder=3, color=\"gray\", marker='o')\n",
    "ax7.plot(test.index, test[\"Percentage\"], zorder=3, color=\"#0e4f66\", marker='o')\n",
    "ax7.text(-1.5, 20.5, 'Years Experience', fontsize=14, fontweight='bold', fontfamily='serif', color=\"#323232\")\n",
    "ax7.set_yticklabels(labels = ['0   ', '5%','10%','15%'])\n",
    "ax7.xaxis.set_major_locator(mtick.MultipleLocator(5))\n",
    "ax7.yaxis.set_major_locator(mtick.MultipleLocator(5))\n",
    "\n",
    "\n",
    "# Ax8 - MAJOR DISCIPLINE\n",
    "train = pd.DataFrame(train_df[\"major_discipline\"].value_counts())\n",
    "train[\"Percentage\"] = train[\"major_discipline\"].apply(lambda x: x/sum(train[\"major_discipline\"])*100)\n",
    "test = pd.DataFrame(test_df[\"major_discipline\"].value_counts())\n",
    "test[\"Percentage\"] = test[\"major_discipline\"].apply(lambda x: x/sum(test[\"major_discipline\"])*100)\n",
    "\n",
    "ax8.barh(np.arange(len(train.index)), train[\"Percentage\"], zorder=3, color=\"gray\", height=0.4)\n",
    "ax8.barh(np.arange(len(test.index))+0.4, test[\"Percentage\"], zorder=3, color=\"#0e4f66\", height=0.4)\n",
    "ax8.text(-5, -0.8, 'Major Discipline', fontsize=14, fontweight='bold', fontfamily='serif', color=\"#323232\")\n",
    "ax8.xaxis.set_major_formatter(mtick.PercentFormatter())\n",
    "ax8.yaxis.set_major_locator(mtick.MultipleLocator(1))\n",
    "ax8.set_yticks(np.arange(len(test.index))+0.4 / 2)\n",
    "ax8.set_yticklabels(list(test.index))\n",
    "ax8.invert_yaxis()\n",
    "\n",
    "\n",
    "\n",
    "for i in range(0,9):\n",
    "    locals()[\"ax\"+str(i)].set_facecolor(background_color) \n",
    "    \n",
    "for i in range(0,9):\n",
    "    locals()[\"ax\"+str(i)].tick_params(axis=u'both', which=u'both',length=0)\n",
    "\n",
    "\n",
    "for s in [\"top\",\"right\",\"left\"]:\n",
    "    for i in range(0,9):\n",
    "        locals()[\"ax\"+str(i)].spines[s].set_visible(False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_palette=[\"gray\",\"#0e4f66\"]\n",
    "fig = plt.figure(figsize=(18,15), dpi=150)\n",
    "fig.patch.set_facecolor(background_color) # figure background color\n",
    "gs = fig.add_gridspec(3, 3)\n",
    "gs.update(wspace=0.4, hspace=0.6)\n",
    "ax0 = fig.add_subplot(gs[0, 0])\n",
    "ax1 = fig.add_subplot(gs[0, 1])\n",
    "ax2 = fig.add_subplot(gs[1, 0])\n",
    "ax3 = fig.add_subplot(gs[1, 1])\n",
    "ax4 = fig.add_subplot(gs[2, 0])\n",
    "ax5 = fig.add_subplot(gs[2, 1])\n",
    "\n",
    "# Distribution\n",
    "ax0.text(-1, 19000, 'Who is looking for a new job?', fontsize=20, fontweight='bold', fontfamily='serif', color=\"#323232\")\n",
    "ax0.text(-1, 17500, 'Most job-seekers appear to be male', fontsize=14, fontweight='light', fontfamily='serif', color=\"#323232\")\n",
    "ax0.text(-1, 14050, 'Overall', fontsize=14, fontweight='bold', fontfamily='serif', color=\"#323232\")\n",
    "sns.countplot(x=train_df[\"gender\"], color=\"#247747\", ax=ax0, zorder=3,alpha=0.9)\n",
    "\n",
    "\n",
    "\n",
    "# Gender\n",
    "ax1.text(-1, 11000, 'Job searching by gender', fontsize=14, fontweight='bold', fontfamily='serif', color=\"#323232\")\n",
    "sns.countplot(x=\"gender\", hue=\"target\", data=train_df, palette=color_palette, ax=ax1, zorder=3)\n",
    "legend_labels, _= ax1.get_legend_handles_labels()\n",
    "ax1.legend(legend_labels, [\"Non-Job Seeker\", \"Job Seeker\"], ncol=2, bbox_to_anchor=(-0.52, 1.28), facecolor=background_color, edgecolor=background_color)\n",
    "\n",
    "\n",
    "\n",
    "# CDI\n",
    "ax2.text(0.3, 16, 'Does the City Development Index play a role?', fontsize=20, fontweight='bold', fontfamily='serif', color=\"#323232\")\n",
    "ax2.text(0.3, 14.5, 'Interestingly, we see Job Seekers are frequently from cities with a lower CDI score', fontsize=14, fontweight='light', fontfamily='serif', color=\"#323232\")\n",
    "ax2.text(0.3, 13, 'Overall', fontsize=14, fontweight='bold', fontfamily='serif', color=\"#323232\")\n",
    "sns.kdeplot(train_df[\"city_development_index\"], color=\"#247747\", shade=True, ax=ax2, zorder=3)\n",
    "\n",
    "\n",
    "\n",
    "ax3.text(0.33, 15.5, 'Job Seeker / Non-Job Seeker', fontsize=14, fontweight='bold', fontfamily='serif', color=\"#323232\")\n",
    "sns.kdeplot(train_df.loc[(train_df[\"target\"]==0), \"city_development_index\"], color=\"gray\", label=\"Not Survived\", ax=ax3)\n",
    "sns.kdeplot(train_df.loc[(train_df[\"target\"]==1), \"city_development_index\"], color=\"#0e4f66\", label=\"Survived\", ax=ax3)\n",
    "\n",
    "\n",
    "\n",
    "###\n",
    "aug_train['count'] = 1\n",
    "job_hunt_only = aug_train[aug_train['target']==1]\n",
    "no_job_hunt_only = aug_train[aug_train['target']==0]\n",
    "\n",
    "job_change = aug_train.groupby(['education_level','last_new_job'])['experience'].sum().unstack().loc[ed_order,job_order]\n",
    "\n",
    "job_hunt_only.groupby(['target','last_new_job'])['count'].sum().unstack()\n",
    "notseek_job_change = no_job_hunt_only.groupby(['target','last_new_job'])['count'].sum().unstack().T\n",
    "seek_job_change = job_hunt_only.groupby(['target','last_new_job'])['count'].sum().unstack().T\n",
    "\n",
    "notseek_job_change.columns = ['count']\n",
    "seek_job_change.columns = ['count']\n",
    "\n",
    "notseek_job_change[\"percentage\"] = notseek_job_change[\"count\"].apply(lambda x: x/sum(notseek_job_change[\"count\"])) *100\n",
    "seek_job_change[\"percentage\"] = seek_job_change[\"count\"].apply(lambda x: x/sum(seek_job_change[\"count\"])) *100\n",
    "\n",
    "\n",
    "ed_notseek_job_change = no_job_hunt_only.groupby(['target','education_level'])['count'].sum().unstack().T.loc[ed_order]\n",
    "ed_seek_job_change = job_hunt_only.groupby(['target','education_level'])['count'].sum().unstack().T.loc[ed_order]\n",
    "\n",
    "ed_notseek_job_change.columns = ['count']\n",
    "ed_seek_job_change.columns = ['count']\n",
    "\n",
    "ed_notseek_job_change[\"percentage\"] = ed_notseek_job_change[\"count\"].apply(lambda x: x/sum(ed_notseek_job_change[\"count\"])) *100\n",
    "ed_seek_job_change[\"percentage\"] = ed_seek_job_change[\"count\"].apply(lambda x: x/sum(ed_seek_job_change[\"count\"])) *100\n",
    "\n",
    "###\n",
    "\n",
    "\n",
    "ax4.barh(notseek_job_change.index, notseek_job_change['percentage'], color=\"gray\", zorder=3, height=0.7)\n",
    "ax4.barh(seek_job_change.index, seek_job_change['percentage'], color=\"#0e4f66\", zorder=3, height=0.3)\n",
    "ax4.xaxis.set_major_locator(mtick.MultipleLocator(10))\n",
    "\n",
    "\n",
    "##\n",
    "ax5.barh(ed_notseek_job_change.index, ed_notseek_job_change['percentage'], color=\"gray\", zorder=3, height=0.7)\n",
    "ax5.barh(ed_seek_job_change.index, ed_seek_job_change['percentage'], color=\"#0e4f66\", zorder=3, height=0.3)\n",
    "ax5.xaxis.set_major_locator(mtick.MultipleLocator(10))\n",
    "\n",
    "##\n",
    "ax4.text(-1, 5.7, 'Last job change (yrs)',fontsize=15, fontweight='bold', fontfamily='serif',color='#323232')\n",
    "ax5.text(0, 4.55, 'Education level', fontsize=15, fontweight='bold', fontfamily='serif',color='#323232')\n",
    "\n",
    "ax4.text(-2.5, 7.5, 'Are there other differences?', \n",
    "         fontsize=20, fontweight='bold', fontfamily='serif',color='#323232')\n",
    "\n",
    "ax4.text(-2.5, 6.75, \n",
    "         'We see broadly similar patterns, but notable areas of difference', \n",
    "         fontsize=14, fontweight='light', fontfamily='serif')\n",
    "\n",
    "\n",
    "####\n",
    "\n",
    "fig.text(0.77, 0.89\n",
    "         , 'Insight', fontsize=15, fontweight='bold', fontfamily='serif',color='#323232')\n",
    "\n",
    "fig.text(0.77, 0.39, '''\n",
    "We note that most job-seekers are Male.\n",
    "This is not all that surprising as in this\n",
    "dataset Males make up the majority of the\n",
    "sample population.\n",
    "\n",
    "What is more interesting though is\n",
    "the City Development Index (CDI) chart.\n",
    "There we see that there are two peaks\n",
    "for job-seekers. \n",
    "The peaks are at high and low CDI scores. \n",
    "\n",
    "\n",
    "We can ponder why this might be;\n",
    "in high CDI areas perhaps there are a \n",
    "lot of opportunities and therefore\n",
    "people feel encouraged to seek better roles.\n",
    "\n",
    "\n",
    "Perhaps in lower CDI areas candidates\n",
    "want to improve their circumstances by\n",
    "searching for new jobs, maybe in new areas. \n",
    "\n",
    "\n",
    "This is all conjecture, but interesting\n",
    "nonetheless.\n",
    "\n",
    "It is also interesting to see that job-seekers\n",
    "have changed job more often that non-job seekers\n",
    "within that past 1 year, and also\n",
    "those that have never looked for a job\n",
    "also seem to be ready for a new\n",
    "challenge.\n",
    "'''\n",
    "         , fontsize=14, fontweight='light', fontfamily='serif',color='#323232')\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.lines as lines\n",
    "l1 = lines.Line2D([0.7, 0.7], [0.1, 0.9], transform=fig.transFigure, figure=fig,color='black',lw=0.2)\n",
    "fig.lines.extend([l1])\n",
    "\n",
    "for s in [\"top\",\"right\",\"left\"]:\n",
    "    for i in range(0,6):\n",
    "        locals()[\"ax\"+str(i)].spines[s].set_visible(False)\n",
    "        \n",
    "for i in range(0,6):\n",
    "        locals()[\"ax\"+str(i)].set_facecolor(background_color)\n",
    "        locals()[\"ax\"+str(i)].tick_params(axis=u'both', which=u'both',length=0)\n",
    "        locals()[\"ax\"+str(i)].grid(color='gray', linestyle=':', axis='y', zorder=0,  dashes=(1,5))      \n",
    "\n",
    "        \n",
    "for x in range(0,4):\n",
    "    for y in range(0,4):\n",
    "        locals()[\"ax\"+str(x)].set_xlabel(\"\")\n",
    "        locals()[\"ax\"+str(y)].set_ylabel(\"\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_gen_size = pd.pivot_table(aug_train, values='count',index=['gender'],columns=['company_size'],aggfunc=np.sum).loc[gender_order, size_order]\n",
    "ct_gen_size = pd.crosstab(aug_train['company_size'],aug_train['experience'], normalize='index').loc[size_order,exp_yrs_order_2]\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(15,10), dpi=150) \n",
    "fig.patch.set_facecolor(background_color)\n",
    "gs = fig.add_gridspec(2, 1)\n",
    "gs.update(wspace=0, hspace=-0.09)\n",
    "ax0 = fig.add_subplot(gs[:,:])\n",
    "\n",
    "colors = [\"#fbfbfb\", \"#4b4b4c\",\"#0e4f66\"]\n",
    "colormap = matplotlib.colors.LinearSegmentedColormap.from_list(\"\", colors)\n",
    "\n",
    "sns.heatmap(ax=ax0, data=ct_gen_size, linewidths=.1, vmin=0, vmax=0.075,\n",
    "            square=True, cbar=False, cmap=colormap,linewidth=3, annot=True, fmt='1.0%',annot_kws={\"fontsize\":14})\n",
    "\n",
    " \n",
    "ax0.set_facecolor(background_color) \n",
    "ax0.set_xlabel(\"Employee Experience [Years]\",fontfamily='serif',fontsize=12,loc='left')\n",
    "ax0.set_ylabel(\"\")\n",
    "\n",
    "for s in [\"top\",\"right\",\"left\"]:\n",
    "    ax0.spines[s].set_visible(False)\n",
    "\n",
    "    \n",
    "ax0.text(0, -1.4, \n",
    "         'Company size & employee experience', \n",
    "         fontsize=20, \n",
    "         fontweight='bold', \n",
    "         fontfamily='serif',\n",
    "        )\n",
    "\n",
    "ax0.text(0, -0.9, \n",
    "         'Those with over 20 yrs experience dominate the work force at all company sizes.', \n",
    "         fontsize=13, \n",
    "         fontweight='light', \n",
    "         fontfamily='serif',\n",
    "        )\n",
    "ax0.text(0, -0.5, \n",
    "         'We also observe some heat around the lower experience range and at smaller companies.', \n",
    "         fontsize=13, \n",
    "         fontweight='light', \n",
    "         fontfamily='serif',\n",
    "        )\n",
    "\n",
    "ax0.tick_params(axis=u'both', which=u'both',length=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_hunt_only = aug_train[aug_train['target']==1]\n",
    "no_job_hunt_only = aug_train[aug_train['target']==0]\n",
    "\n",
    "job_seek = pd.crosstab(job_hunt_only['company_size'],job_hunt_only['experience'], normalize='index').loc[size_order,exp_yrs_order_2]\n",
    "no_seek = pd.crosstab(no_job_hunt_only['company_size'],no_job_hunt_only['experience'], normalize='index').loc[size_order,exp_yrs_order_2]\n",
    "\n",
    "###\n",
    "fig = plt.figure(figsize=(14,14),dpi=150)\n",
    "fig.patch.set_facecolor(background_color)\n",
    "gs = fig.add_gridspec(2, 3)\n",
    "gs.update(wspace=0.2, hspace=0.3)\n",
    "ax0 = fig.add_subplot(gs[0,:])\n",
    "ax1 = fig.add_subplot(gs[1,:])\n",
    "\n",
    "\n",
    "colors = [\"#fbfbfb\", \"#4b4b4c\",\"#0e4f66\"]\n",
    "colormap = matplotlib.colors.LinearSegmentedColormap.from_list(\"\", colors)\n",
    "\n",
    "sns.heatmap(ax=ax0, data=job_seek, linewidths=.1, vmin=0, vmax=0.075,\n",
    "            square=True, cbar_kws={\"orientation\": \"horizontal\"}, cbar=False, cmap=colormap,linewidth=3, annot=False, fmt='1.0%',annot_kws={\"fontsize\":14})\n",
    "\n",
    "sns.heatmap(ax=ax1, data=no_seek, linewidths=.1, vmin=0, vmax=0.075,\n",
    "            square=True, cbar_kws={\"orientation\": \"horizontal\"}, cbar=False, cmap=colormap,linewidth=3, annot=False, fmt='1.0%',annot_kws={\"fontsize\":14})\n",
    "\n",
    "ax0.set_facecolor(background_color) \n",
    "ax0.set_xlabel(\"\")\n",
    "ax0.set_ylabel(\"\")\n",
    "\n",
    "for s in [\"top\",\"right\",\"left\"]:\n",
    "    ax0.spines[s].set_visible(False)\n",
    "\n",
    "    \n",
    "ax0.set_xlabel(\"\")\n",
    "ax0.set_ylabel(\"\")\n",
    "ax1.set_xlabel(\"Employee Experience [Years]\",fontfamily='serif',fontsize=14,loc='left')\n",
    "ax1.set_ylabel(\"\")\n",
    "\n",
    "for s in [\"top\",\"right\",\"left\"]:\n",
    "    ax0.spines[s].set_visible(False)\n",
    "    ax1.spines[s].set_visible(False)\n",
    "\n",
    "    \n",
    "ax0.text(0, -1.3, \n",
    "         'Job Seekers: Company size & employee experience', \n",
    "         fontsize=20, \n",
    "         fontweight='bold', \n",
    "         fontfamily='serif',\n",
    "        )\n",
    "\n",
    "ax1.text(0, -0.7, \n",
    "         'Those with over 20 yrs experience dominate the work force at all company sizes.', \n",
    "         fontsize=13, \n",
    "         fontweight='light', \n",
    "         fontfamily='serif',\n",
    "        )\n",
    "ax1.text(0, -0.35, \n",
    "         'People with 20+ years of experience are not seeking new roles; perhaps they are at their desired seniority level.', \n",
    "         fontsize=13, \n",
    "         fontweight='light', \n",
    "         fontfamily='serif',\n",
    "        )\n",
    "    \n",
    "ax1.text(0, -1.3, \n",
    "         'Non-Job Seekers: Company size & employee experience', \n",
    "         fontsize=20, \n",
    "         fontweight='bold', \n",
    "         fontfamily='serif',\n",
    "        )\n",
    "\n",
    "ax0.text(0, -0.7, \n",
    "         'We see that most job seekers have between 2 - 6 years experience.', \n",
    "         fontsize=13, \n",
    "         fontweight='light', \n",
    "         fontfamily='serif',\n",
    "        )\n",
    "ax0.text(0, -0.35, \n",
    "         'Anecdotally this seems correct; people build their skills in the first few years of their career then seek new challenges.', \n",
    "         fontsize=13, \n",
    "         fontweight='light', \n",
    "         fontfamily='serif',\n",
    "        )\n",
    "\n",
    "ax0.tick_params(axis=u'both', which=u'both',length=0)\n",
    "ax1.tick_params(axis=u'both', which=u'both',length=0)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_hunt_only = aug_train[aug_train['target']==1]\n",
    "no_job_hunt_only = aug_train[aug_train['target']==0]\n",
    "\n",
    "job_seek = pd.crosstab(job_hunt_only['education_level'],job_hunt_only['company_type'], normalize='index').loc[ed_order,company_order]\n",
    "no_seek = pd.crosstab(no_job_hunt_only['education_level'],no_job_hunt_only['company_type'], normalize='index').loc[ed_order,company_order]\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(15,15),dpi=150)\n",
    "fig.patch.set_facecolor(background_color)\n",
    "\n",
    "gs = fig.add_gridspec(1, 2)\n",
    "gs.update(wspace=0.05, hspace=0.3)\n",
    "ax0 = fig.add_subplot(gs[0,0])\n",
    "ax1 = fig.add_subplot(gs[0,1])\n",
    "\n",
    "\n",
    "colors = [\"#fbfbfb\", \"#4b4b4c\",\"#0e4f66\"]\n",
    "colormap = matplotlib.colors.LinearSegmentedColormap.from_list(\"\", colors)\n",
    "\n",
    "sns.heatmap(ax=ax0, data=job_seek, linewidths=.1, vmin=-0.01, vmax=0.075,\n",
    "            square=True, cbar_kws={\"orientation\": \"horizontal\"}, cbar=False, cmap=colormap,linewidth=3, annot=True, fmt='1.0%',annot_kws={\"fontsize\":14})\n",
    "\n",
    "sns.heatmap(ax=ax1, data=no_seek, linewidths=.1, vmin=-0.01, vmax=0.075,\n",
    "            square=True, cbar_kws={\"orientation\": \"horizontal\"}, cbar=False,yticklabels=False, cmap=colormap,linewidth=3, annot=True, fmt='1.0%',annot_kws={\"fontsize\":14})\n",
    "\n",
    "\n",
    "ax0.set_facecolor(background_color)\n",
    "ax1.set_facecolor(background_color) \n",
    "ax0.set_xlabel(\"\")\n",
    "ax0.set_ylabel(\"\")\n",
    "\n",
    "for s in [\"top\",\"right\",\"left\"]:\n",
    "    ax0.spines[s].set_visible(False)\n",
    "\n",
    "\n",
    "ax1.set_xlabel(\"\")\n",
    "ax1.set_ylabel(\"\")\n",
    "\n",
    "for s in [\"top\",\"right\",\"left\"]:\n",
    "    ax0.spines[s].set_visible(False)\n",
    "    ax1.spines[s].set_visible(False)\n",
    "    \n",
    "    \n",
    "ax0.text(0, -1, \n",
    "         'Education level & company type', \n",
    "         fontsize=20, \n",
    "         fontweight='bold', \n",
    "         fontfamily='serif',\n",
    "        )\n",
    "\n",
    "ax0.text(0, -0.6, \n",
    "         'Job Seekers', \n",
    "         fontsize=15, \n",
    "         fontweight='bold', \n",
    "         fontfamily='serif',\n",
    "        )\n",
    "\n",
    "ax0.text(0, -0.2, \n",
    "         'Seem to be at earlier stages of their education.', \n",
    "         fontsize=13, \n",
    "         fontweight='light', \n",
    "         fontfamily='serif',\n",
    "        )\n",
    "\n",
    "\n",
    "###\n",
    "\n",
    "ax1.text(0, -0.6, \n",
    "         'Non-Job Seekers', \n",
    "         fontsize=15, \n",
    "         fontweight='bold', \n",
    "         fontfamily='serif',\n",
    "        )\n",
    "ax1.text(0, -0.2, \n",
    "         'We see a slightly higher education level.', \n",
    "         fontsize=13, \n",
    "         fontweight='light', \n",
    "         fontfamily='serif',\n",
    "        )\n",
    "    \n",
    "\n",
    "ax0.tick_params(axis=u'both', which=u'both',length=0)\n",
    "ax1.tick_params(axis=u'both', which=u'both',length=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Orders\n",
    "#ed_order = ['Primary School','High School','Graduate','Masters','Phd']\n",
    "#enroll_order = ['No Enrollment','Part time course','Full time course']\n",
    "#disc_order = ['STEM','Unknown','Humanities','Other','Business Degree','Arts','No Major']\n",
    "#exp_yrs_order = ['0','1','2','3','4','5','6','7','8','9','10','11','12','13','14','15','16','17','18','19','20']\n",
    "#size_order = ['0','<10', '10-49', '50-99', '100-500', '500-999', '1000-4999', '5000-9999', '10000+']\n",
    "#job_order = ['Never', '1', '2', '3', '4', '>4']\n",
    "#exp_order =['No relevant experience','Has relevant experience']\n",
    "#gender_order = ['Male','Female','Other','Not provided']\n",
    "#company_order = ['Pvt Ltd','Unknown','Funded Startup','Public Sector','Early Stage Startup','NGO','Other']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = train_df\n",
    "data['count'] = 1\n",
    "\n",
    "data_plot = pd.pivot_table(data, values='count', index=['major_discipline'], columns=['company_size'], aggfunc=np.sum).fillna(0).astype(int).stack().loc[disc_order, size_order]\n",
    "data_job_seek = pd.pivot_table(data[data['target']==1], values='count', index=['major_discipline'], columns=['company_size'], aggfunc=np.sum).fillna(0).astype(int).stack().loc[disc_order, size_order]\n",
    "data_no_job_seek = pd.pivot_table(data[data['target']==0], values='count', index=['major_discipline'], columns=['company_size'], aggfunc=np.sum).fillna(0).astype(int).stack().loc[disc_order, size_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/56337732/how-to-plot-scatter-pie-chart-using-matplotlib\n",
    "def drawPieMarker(xs, ys, ratios, sizes, colors, ax):\n",
    "    markers = []\n",
    "    previous = 0\n",
    "    # calculate the points of the pie pieces\n",
    "    for color, ratio in zip(colors, ratios):\n",
    "        this = 2 * np.pi * ratio + previous\n",
    "        x  = [0] + np.cos(np.linspace(previous, this, 30)).tolist() + [0]\n",
    "        y  = [0] + np.sin(np.linspace(previous, this, 30)).tolist() + [0]\n",
    "        xy = np.column_stack([x, y])\n",
    "        previous = this\n",
    "        markers.append({'marker':xy, 's':np.abs(xy).max()**2*np.array(sizes), 'facecolor':color})\n",
    "\n",
    "    # scatter each of the pie pieces to create pies\n",
    "    for marker in markers:\n",
    "        ax.scatter(xs, ys, **marker, alpha=0.9, ec=background_color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot inspired by Subin An\n",
    "\n",
    "fig = plt.figure(figsize=(13, 13), dpi=150)\n",
    "gs = fig.add_gridspec(5, 5)\n",
    "gs.update(wspace=0.1, hspace=0.1)\n",
    "fig.patch.set_facecolor(background_color) \n",
    "\n",
    "# Pie \n",
    "\n",
    "ax_centre = fig.add_subplot(gs[1:4, 0:4]) \n",
    "for cl_idx in disc_order[::-1]:\n",
    "    for age_idx in size_order:\n",
    "        seek = data_job_seek[cl_idx][age_idx]\n",
    "        no_seek = data_no_job_seek[cl_idx][age_idx]\n",
    "        total = data_job_seek[cl_idx][age_idx]\n",
    "        drawPieMarker([age_idx],[cl_idx], [seek/(seek+no_seek), no_seek/(seek+no_seek)] ,[total*2.5], [\"#0e4f66\", \"gray\"], ax=ax_centre)\n",
    "\n",
    "ax_centre.grid(linewidth=0.1)        \n",
    "ax_centre.set_facecolor(background_color)\n",
    "ax_centre.set_xticklabels(size_order,fontfamily='serif', fontsize=11, rotation=90)\n",
    "\n",
    " # Top\n",
    "ax_top = fig.add_subplot(gs[0, :4], sharex=ax_centre) \n",
    "c_size_non = data[data['target']==0]['company_size'].value_counts()[size_order]\n",
    "ax_top.bar(c_size_non.index, c_size_non, width=0.45, alpha=0.9,ec=background_color, color='gray')\n",
    "\n",
    "c_size = data[data['target']==1]['company_size'].value_counts()[size_order]\n",
    "ax_top.bar(c_size.index, c_size, bottom=c_size_non , width=0.45, alpha=0.9, ec=background_color,color='#0e4f66')\n",
    "\n",
    "plt.setp(ax_top.get_xticklabels(), visible=False)\n",
    "ax_top.set_facecolor(background_color)\n",
    "\n",
    "# Side \n",
    "ax_side = fig.add_subplot(gs[1:4, 4], sharey=ax_centre) \n",
    "disc_no = data[data['target']==0]['major_discipline'].value_counts()[disc_order]\n",
    "ax_side.barh(disc_no.index[::-1], disc_no[::-1], height=0.55, alpha=0.9,ec=background_color, color='gray')\n",
    "\n",
    "disc_yes = data[data['target']==1]['major_discipline'].value_counts()[disc_order]\n",
    "ax_side.barh(disc_yes.index[::-1], disc_yes[::-1], left= disc_no[::-1],height=0.55, alpha=0.9, ec=background_color,color='#0e4f66')\n",
    "\n",
    "plt.setp(ax_side.get_yticklabels(), visible=False)\n",
    "ax_side.set_facecolor(background_color)\n",
    "\n",
    "# Spines\n",
    "for s in ['top', 'left', 'right', 'bottom']:\n",
    "    ax_centre.spines[s].set_visible(False)\n",
    "    ax_top.spines[s].set_visible(False)\n",
    "    ax_side.spines[s].set_visible(False)\n",
    "ax_centre.set_axisbelow(True)    \n",
    "\n",
    "fig.text(0.9, 0.9, 'Job seeking, company size, and major discipline', fontweight='bold', fontfamily='serif', fontsize=20, ha='right') \n",
    "fig.text(0.9, 0.87, 'Stacked Bar Chart & Categorical Bubble Pie Chart', fontweight='light', fontfamily='serif', fontsize=13, ha='right')\n",
    "\n",
    "fig.text(0.633,0.84,\"Job Seeking\", fontweight=\"bold\", fontfamily='serif', fontsize=15, color='#0e4f66')\n",
    "fig.text(0.745,0.84,\"|\", fontweight=\"bold\", fontfamily='serif', fontsize=15, color='black')\n",
    "fig.text(0.755,0.84,\"Not Job Seeking\", fontweight=\"bold\", fontfamily='serif', fontsize=15, color='gray')\n",
    "\n",
    "ax_centre.tick_params(axis=u'both', which=u'both',length=0)\n",
    "ax_top.tick_params(axis=u'both', which=u'both',length=0)\n",
    "ax_side.tick_params(axis=u'both', which=u'both',length=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Orders\n",
    "ed_order = ['Primary School','High School','Graduate','Masters','Phd','Unknown']\n",
    "enroll_order = ['No Enrollment','Part time course','Full time course','Unknown']\n",
    "disc_order = ['STEM','Humanities','Business Degree','Arts','Other','No Major','Unknown']\n",
    "exp_yrs_order = ['0','<1','1','2','3','4','5','6','7','8','9','10','11','12','13','14','15','16','17','18','19','20','>20']\n",
    "exp_yrs_order_2 = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]\n",
    "size_order = ['0','<10', '10-49', '50-99', '100-500', '500-999', '1000-4999', '5000-9999', '10000+','Unemployed']\n",
    "job_order = ['Never', '1', '2', '3', '4', '>4','Unknown']\n",
    "exp_order =['No relevant experience','Has relevant experience']\n",
    "gender_order = ['Male','Female','Other']\n",
    "company_order = ['Pvt Ltd','Funded Startup','Public Sector','Early Stage Startup','NGO','Other','Unknown','Unemployed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_change = aug_train.groupby(['education_level','last_new_job'])['experience'].sum().unstack().loc[ed_order,job_order]\n",
    "jc_never = job_change['Never']\n",
    "\n",
    "\n",
    "job_change_norm = pd.crosstab(aug_train['education_level'],aug_train['last_new_job'],normalize='columns').loc[ed_order,job_order,]\n",
    "job_change_norm = round(job_change_norm*100,1).astype(int)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##\n",
    "job_hunt_only = aug_train[aug_train['target']==1]\n",
    "no_job_hunt_only = aug_train[aug_train['target']==0]\n",
    "\n",
    "\n",
    "seekers_job_change_norm = pd.crosstab(job_hunt_only['education_level'],job_hunt_only['last_new_job'],normalize='columns').loc[ed_order,job_order,]\n",
    "seekers_job_change_norm = round(seekers_job_change_norm*100,1).astype(int)\n",
    "seekers_job_change_norm\n",
    "\n",
    "non_seekers_job_change_norm = pd.crosstab(no_job_hunt_only['education_level'],no_job_hunt_only['last_new_job'],normalize='columns').loc[ed_order,job_order,]\n",
    "non_seekers_job_change_norm = round(non_seekers_job_change_norm*100,1).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 새로운 EDA 여기서부터 시작!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_num = train_df[['enrollee_id', 'city_development_index', 'training_hours', 'target']]\n",
    "group_obj = train_df[['gender', 'relevent_experience', 'enrolled_university', 'education_level', 'major_discipline', 'experience', 'company_size',\n",
    "                       'company_type', 'last_new_job']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실수형 변수 자료 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = group_num.corr()\n",
    "\n",
    "# 상관관계 히트맵 그리기\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pip install scikit-learn==1.1\n",
    "pip install imbalanced-learn==0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 여기서 범주형 자료로 변환 들어가야 함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 범주형 변수 전처리 \n",
    "* one-hot encoding : gender, enrolled_university, major_discipline, company_type,relevent_experience\n",
    "* ordinal encoding : education_level, company_size, last_new_job\n",
    "* object -> int : Experience: 수치화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모든 각 열에서의 unique한 value\n",
    "for column in aug_train.columns:\n",
    "    print(f\"{column} unique values: \\n{aug_train[column].unique()}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### scikit-learn 버전 출력\n",
    "print(f\"scikit-learn version: {sklearn.__version__}\")\n",
    "이 코드를 실행하여 scikit-learn의 버전을 확인하십시오. scikit-learn의 버전에 따라 OneHotEncoder의 매개변수 이름이 달라질 수 있습니다.\n",
    "\n",
    "- 1.2.0 이상: sparse_output=False 사용\n",
    "- 1.1.0 이하: sparse=False 사용\n",
    "\n",
    "(버전 바꾸는 방법) 터미널에서 실행\n",
    "-> pip uninstall scikit-learn -y\n",
    "-> pip install scikit-learn==1.1.3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "print(f\"scikit-learn version: {sklearn.__version__}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encoding 함수(dummy)\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "## scikit-learn version: 1.4.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def one_hot_encode_column(df, column_name):\n",
    "    # OneHotEncoder 객체 생성\n",
    "    # sparse_output=False\n",
    "    encoder = OneHotEncoder(drop='first', sparse=False, dtype=int)\n",
    "    # 해당 칼럼에 대해 OneHotEncoding 수행\n",
    "    encoded_data = encoder.fit_transform(df[[column_name]])\n",
    "    \n",
    "    # 새로운 칼럼 이름 생성\n",
    "    new_column_names = encoder.get_feature_names_out([column_name])\n",
    "    \n",
    "    # 새로운 칼럼 데이터프레임 생성\n",
    "    encoded_df = pd.DataFrame(encoded_data, columns=new_column_names, index=df.index)\n",
    "    \n",
    "    # 기존 열 제거 및 새로운 열 추가\n",
    "    df = pd.concat([df, encoded_df], axis=1)\n",
    "    df.drop(columns=[column_name], inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_train = one_hot_encode_column(aug_train, 'gender')\n",
    "aug_train = one_hot_encode_column(aug_train, 'enrolled_university')\n",
    "aug_train = one_hot_encode_column(aug_train, 'major_discipline')\n",
    "aug_train = one_hot_encode_column(aug_train, 'company_type')\n",
    "aug_train = one_hot_encode_column(aug_train, 'relevent_experience')\n",
    "\n",
    "aug_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ordinal encoding 함수\n",
    "\n",
    "def ordinal_encode_education_level(x):\n",
    "    education_level_mapping = {'Primary School': 1, 'High School': 2, 'Graduate': 3, 'Masters': 4, 'Phd': 5, 'Unknown': 0}\n",
    "    return education_level_mapping.get(x, x)\n",
    "\n",
    "def ordinal_encode_company_size(x):\n",
    "    company_size_mapping = {'<10': 1, '10-49': 2, '50-99': 3, '100-500': 4, '500-999': 5, '1000-4999': 6, '5000-9999': 7, '10000+': 8, 'Unknown': 0, 'Unemployed': 9}\n",
    "    return company_size_mapping.get(x, x)\n",
    "\n",
    "def ordinal_encode_last_new_job(x):\n",
    "    last_new_job_mapping = {'Never': 1, '1': 2, '2': 3, '3': 4, '4': 5, '>4': 6, 'Unknown': 0}\n",
    "    return last_new_job_mapping.get(x, x)\n",
    "\n",
    "# education_level 칼럼에 ordinal_encode_education_level 함수를 적용하여 변환\n",
    "aug_train['education_level'] = aug_train['education_level'].apply(ordinal_encode_education_level)\n",
    "\n",
    "# company_size 칼럼에 ordinal_encode_company_size 함수를 적용하여 변환\n",
    "aug_train['company_size'] = aug_train['company_size'].apply(ordinal_encode_company_size)\n",
    "\n",
    "# last_new_job 칼럼에 ordinal_encode_last_new_job 함수를 적용하여 변환\n",
    "aug_train['last_new_job'] = aug_train['last_new_job'].apply(ordinal_encode_last_new_job)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unknown은 사실 ordinal에 들어가면 안되는데, 결측처리를 안했기 때문에 가지고 감\n",
    "# 따라서 unknown인지 아닌지를 구분할 수 있는 더미칼럼을 생성\n",
    "\n",
    "# 'education_level' 열을 기준으로 x가 0면 1, 아니면 0으로 'education_level_Unknown' 열 생성\n",
    "aug_train['education_level_Unknown'] = aug_train['education_level'].apply(lambda x: 1 if x == 0 else 0)\n",
    "\n",
    "# 'company_size' 열을 기준으로 x가 0이면 1, 아니면 0으로 'company_size_Unknown' 열 생성\n",
    "aug_train['company_size_Unknown'] = aug_train['company_size'].apply(lambda x: 1 if x == 0 else 0)\n",
    "\n",
    "# 'company_size' 열을 기준으로 x가 9이면 1, 아니면 0으로 'company_size_Unknown' 열 생성\n",
    "aug_train['company_size_Unemployed'] = aug_train['company_size'].apply(lambda x: 1 if x == 9 else 0)\n",
    "\n",
    "# 'last_new_job' 열을 기준으로 x가 0이면 1, 아니면 0으로 'last_new_job_Unknown' 열 생성\n",
    "aug_train['last_new_job_Unknown'] = aug_train['last_new_job'].apply(lambda x: 1 if x == 0 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 더미를 만드는 대신 원래 있던 열에서는 0값으로 변경(추가된 코드)\n",
    "# 'education_level'가 0인 행의 'education_level'를 0로 변경\n",
    "aug_train.loc[aug_train['education_level'] == 0, 'education_level'] = 0\n",
    "\n",
    "# 'company_size'가 0인 행의 'education_level'를 0로 변경\n",
    "aug_train.loc[aug_train['company_size'] == 0, 'company_size'] = 0\n",
    "\n",
    "# 'company_size'가 9인 행의 'education_level'를 0로 변경\n",
    "aug_train.loc[aug_train['company_size'] == 9, 'company_size'] = 0\n",
    "\n",
    "# 'last_new_job'가 0인 행의 'last_new_job'를 0로 변경\n",
    "aug_train.loc[aug_train['last_new_job'] == 0, 'last_new_job'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_train['company_size'] = aug_train['company_size'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(aug_train['education_level'].unique())\n",
    "print(aug_train['company_size'].unique())\n",
    "print(aug_train['last_new_job'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_train[['company_size_Unemployed','company_type_Unemployed']].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_train.drop(columns='company_size_Unemployed',axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 모델 전에 마지막 작업"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_train = aug_train.drop(columns=['enrollee_id','count'], axis=1)\n",
    "\n",
    "# Final look at our df\n",
    "aug_train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 돌리기 전에 최종 데이터 셋 정보\n",
    "aug_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = aug_train.dropna().drop(columns=['target']).values\n",
    "y = aug_train.dropna()['target'].values\n",
    "\n",
    "# train, val, test 셋 분리하는 코드\n",
    "# 데이터셋 분할: Train/Validation + Test\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=80)\n",
    "# 데이터셋 분할: Train + Validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.2, random_state=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 열 이름 설정\n",
    "feature_columns = aug_train.drop(columns=['target']).columns\n",
    "target_column = ['target']\n",
    "\n",
    "# numpy 배열을 DataFrame으로 변환\n",
    "X_train_df = pd.DataFrame(X_train_val, columns=feature_columns)\n",
    "X_val_df = pd.DataFrame(X_val, columns=feature_columns)\n",
    "X_test_df = pd.DataFrame(X_test, columns=feature_columns)\n",
    "y_train_df = pd.DataFrame(y_train_val, columns=target_column)\n",
    "y_val_df = pd.DataFrame(y_val, columns=target_column)\n",
    "y_test_df = pd.DataFrame(y_test, columns=target_column)\n",
    "\n",
    "# Ensure 'train_df' and 'test_df' are created correctly by concatenating X and y\n",
    "# train_df : for visuals\n",
    "train_df = pd.concat([X_train_df, y_train_df], axis=1)\n",
    "test_df = pd.concat([X_val_df, y_val_df], axis=1)\n",
    "new_data_df = pd.concat([X_test_df, y_test_df], axis=1)\n",
    "\n",
    "# 출력 확인 \n",
    "print(train_df.info())\n",
    "print(test_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = X_train, X_val, y_train, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, roc_auc_score, f1_score, confusion_matrix\n",
    "\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linearSVC = LinearSVC(dual=False, random_state=123, max_iter=10000)\n",
    "linearSVC.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# 예측 및 평가\n",
    "linearSVC_prediction = linearSVC.predict(X_test_scaled)\n",
    "cm_linearSVC = confusion_matrix(y_test, linearSVC_prediction)\n",
    "df_linearSVC = pd.DataFrame(data=[f1_score(y_test, linearSVC_prediction), recall_score(y_test, linearSVC_prediction),\n",
    "                   precision_score(y_test, linearSVC_prediction), accuracy_score(y_test, linearSVC_prediction), \n",
    "                   roc_auc_score(y_test, linearSVC_prediction)],\n",
    "             columns=['LinearSVC Score'],\n",
    "             index=[\"F1 Score\", \"Recall\", \"Precision\", \"Accuracy\", \"ROC AUC Score\"])\n",
    "df_linearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_grid = {\n",
    "    \"C\": [10 ** k for k in range(-3, 4)], # 1.0\n",
    "    \"penalty\": ['l2', 'l1'], # l2\n",
    "    # \"tol\": [1e-4, 1e-3, 1e-2] # 1e-4\n",
    "}\n",
    "\n",
    "# Initialize the GridSearchCV with LinearSVC\n",
    "linearSVC_t = GridSearchCV(linearSVC, params_grid, cv=5, scoring='f1')\n",
    "linearSVC_t.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Output the best parameters\n",
    "print(\"Best Parameters: \", linearSVC_t.best_params_)\n",
    "\n",
    "# Predictions and evaluation\n",
    "linearSVC_t_prediction = linearSVC_t.predict(X_test_scaled)\n",
    "cm_linearSVC_t = confusion_matrix(y_test, linearSVC_t_prediction)\n",
    "df_linearSVC_t = pd.DataFrame(data=[f1_score(y_test, linearSVC_t_prediction), recall_score(y_test, linearSVC_t_prediction),\n",
    "                   precision_score(y_test, linearSVC_t_prediction), accuracy_score(y_test, linearSVC_t_prediction), \n",
    "                   roc_auc_score(y_test, linearSVC_t_prediction)],\n",
    "             columns=['Tuned LinearSVC Score'],\n",
    "             index=[\"F1 Score\", \"Recall\", \"Precision\", \"Accuracy\", \"ROC AUC Score\"])\n",
    "df_linearSVC_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVC_sigmoid = SVC(kernel='sigmoid')\n",
    "SVC_sigmoid.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# 예측 및 평가\n",
    "SVC_sigmoid_prediction = SVC_sigmoid.predict(X_test_scaled)\n",
    "cm_SVC_sigmoid = confusion_matrix(y_test, SVC_sigmoid_prediction)\n",
    "df_SVC_sigmoid = pd.DataFrame(data=[f1_score(y_test, SVC_sigmoid_prediction), recall_score(y_test, SVC_sigmoid_prediction),\n",
    "                   precision_score(y_test, SVC_sigmoid_prediction), accuracy_score(y_test, SVC_sigmoid_prediction), \n",
    "                   roc_auc_score(y_test, SVC_sigmoid_prediction)],\n",
    "             columns=['SVC Sigmoid Kernel Score'],\n",
    "             index=[\"F1 Score\", \"Recall\", \"Precision\", \"Accuracy\", \"ROC AUC Score\"])\n",
    "df_SVC_sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_grid = {\n",
    "    'C': [10 ** k for k in range(-3, 3)], # 1.0\n",
    "    'gamma': ['scale', 'auto', 0.01, 0.1, 1] # scale\n",
    "}\n",
    "\n",
    "# Initialize the GridSearchCV\n",
    "SVC_sigmoid_t = GridSearchCV(SVC_sigmoid, params_grid, cv=5, scoring='f1')\n",
    "SVC_sigmoid_t.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Output the best parameters\n",
    "print(\"Best Parameters: \", SVC_sigmoid_t.best_params_)\n",
    "\n",
    "# Predictions and evaluation\n",
    "SVC_sigmoid_t_prediction = SVC_sigmoid_t.predict(X_test_scaled)\n",
    "cm_SVC_sigmoid_t = confusion_matrix(y_test, SVC_sigmoid_t_prediction)\n",
    "df_SVC_sigmoid_t = pd.DataFrame(data=[f1_score(y_test, SVC_sigmoid_t_prediction), recall_score(y_test, SVC_sigmoid_t_prediction),\n",
    "                   precision_score(y_test, SVC_sigmoid_t_prediction), accuracy_score(y_test, SVC_sigmoid_t_prediction), \n",
    "                   roc_auc_score(y_test, SVC_sigmoid_t_prediction)],\n",
    "             columns=['Tuned SVC Sigmoid Kernel Score'],\n",
    "             index=[\"F1 Score\", \"Recall\", \"Precision\", \"Accuracy\", \"ROC AUC Score\"])\n",
    "df_SVC_sigmoid_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVC_poly = SVC(kernel='poly')\n",
    "SVC_poly.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# 예측 및 평가\n",
    "SVC_poly_prediction = SVC_poly.predict(X_test_scaled)\n",
    "cm_SVC_poly = confusion_matrix(y_test, SVC_poly_prediction)\n",
    "df_SVC_poly = pd.DataFrame(data=[f1_score(y_test, SVC_poly_prediction), recall_score(y_test, SVC_poly_prediction),\n",
    "                   precision_score(y_test, SVC_poly_prediction), accuracy_score(y_test, SVC_poly_prediction), \n",
    "                   roc_auc_score(y_test, SVC_poly_prediction)],\n",
    "             columns=['SVC Poly Kernel Score'],\n",
    "             index=[\"F1 Score\", \"Recall\", \"Precision\", \"Accuracy\", \"ROC AUC Score\"])\n",
    "df_SVC_poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_grid = {\n",
    "    'C': [10 ** k for k in range(-3, 3)],    \n",
    "    'gamma': ['scale', 'auto', 0.01, 0.1, 1],\n",
    "    # 'degree': [2, 3, 4, 5] # 3\n",
    "}\n",
    "\n",
    "# Initialize the GridSearchCV\n",
    "SVC_poly_t = GridSearchCV(SVC_poly, params_grid, cv=5, scoring='f1')\n",
    "SVC_poly_t.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Output the best parameters\n",
    "print(\"Best Parameters: \", SVC_poly_t.best_params_)\n",
    "\n",
    "# Predictions and evaluation\n",
    "SVC_poly_t_prediction = SVC_poly_t.predict(X_test_scaled)\n",
    "cm_SVC_poly_t = confusion_matrix(y_test, SVC_poly_t_prediction)\n",
    "df_SVC_poly_t = pd.DataFrame(data=[f1_score(y_test, SVC_poly_t_prediction), recall_score(y_test, SVC_poly_t_prediction),\n",
    "                   precision_score(y_test, SVC_poly_t_prediction), accuracy_score(y_test, SVC_poly_t_prediction), \n",
    "                   roc_auc_score(y_test, SVC_poly_t_prediction)],\n",
    "             columns=['Tuned SVC Poly Kernel Score'],\n",
    "             index=[\"F1 Score\", \"Recall\", \"Precision\", \"Accuracy\", \"ROC AUC Score\"])\n",
    "df_SVC_poly_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RBF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVC_rbf = SVC(kernel='rbf')\n",
    "SVC_rbf.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# 예측 및 평가\n",
    "SVC_RBF_prediction = SVC_rbf.predict(X_test_scaled)\n",
    "cm_SVC_RBF = confusion_matrix(y_test, SVC_RBF_prediction)\n",
    "df_SVC_RBF = pd.DataFrame(data=[f1_score(y_test, SVC_RBF_prediction), recall_score(y_test, SVC_RBF_prediction),\n",
    "                   precision_score(y_test, SVC_RBF_prediction), accuracy_score(y_test, SVC_RBF_prediction), \n",
    "                   roc_auc_score(y_test, SVC_RBF_prediction)],\n",
    "             columns=['SVC RBF Kernel Score'],\n",
    "             index=[\"F1 Score\", \"Recall\", \"Precision\", \"Accuracy\", \"ROC AUC Score\"])\n",
    "df_SVC_RBF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_grid = {\n",
    "    'C': [10 ** k for k in range(-3, 3)],    \n",
    "    'gamma': ['scale', 'auto', 0.01, 0.1, 1]\n",
    "}\n",
    "\n",
    "# Initialize the GridSearchCV\n",
    "SVC_rbf_t = GridSearchCV(SVC_rbf, params_grid, cv=5, scoring='f1')\n",
    "SVC_rbf_t.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Output the best parameters\n",
    "print(\"Best Parameters: \", SVC_rbf_t.best_params_)\n",
    "\n",
    "# Predictions and evaluation\n",
    "SVC_RBF_t_prediction = SVC_rbf_t.predict(X_test_scaled)\n",
    "cm_SVC_RBF_t = confusion_matrix(y_test, SVC_RBF_t_prediction)\n",
    "df_SVC_RBF_t = pd.DataFrame(data=[f1_score(y_test, SVC_RBF_t_prediction), recall_score(y_test, SVC_RBF_t_prediction),\n",
    "                   precision_score(y_test, SVC_RBF_t_prediction), accuracy_score(y_test, SVC_RBF_t_prediction), \n",
    "                   roc_auc_score(y_test, SVC_RBF_t_prediction)],\n",
    "             columns=['Tuned SVC RBF Kernel Score'],\n",
    "             index=[\"F1 Score\", \"Recall\", \"Precision\", \"Accuracy\", \"ROC AUC Score\"])\n",
    "df_SVC_RBF_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVC_linear = SVC(kernel='linear')\n",
    "SVC_linear.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# 예측 및 평가\n",
    "SVC_linear_prediction = SVC_linear.predict(X_test_scaled)\n",
    "cm_SVC_linear = confusion_matrix(y_test, SVC_linear_prediction)\n",
    "df_SVC_linear = pd.DataFrame(data=[f1_score(y_test, SVC_linear_prediction), recall_score(y_test, SVC_linear_prediction),\n",
    "                   precision_score(y_test, SVC_linear_prediction), accuracy_score(y_test, SVC_linear_prediction), \n",
    "                   roc_auc_score(y_test, SVC_linear_prediction)],\n",
    "             columns=['SVC Linear Kernel Score'],\n",
    "             index=[\"F1 Score\", \"Recall\", \"Precision\", \"Accuracy\", \"ROC AUC Score\"])\n",
    "df_SVC_linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_grid = {\n",
    "    'C': [10 ** k for k in range(-3, 3)],\n",
    "}\n",
    "\n",
    "# Initialize the GridSearchCV\n",
    "SVC_linear_t = GridSearchCV(SVC_linear, params_grid, cv=5, scoring='f1')\n",
    "SVC_linear_t.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Output the best parameters\n",
    "print(\"Best Parameters: \", SVC_linear_t.best_params_)\n",
    "\n",
    "# Predictions and evaluation\n",
    "SVC_linear_t_prediction = clf.predict(X_test_scaled)\n",
    "cm_SVC_linear_t = confusion_matrix(y_test, SVC_linear_t_prediction)\n",
    "df_SVC_linear_t = pd.DataFrame(data=[f1_score(y_test, SVC_linear_t_prediction), recall_score(y_test, SVC_linear_t_prediction),\n",
    "                   precision_score(y_test, SVC_linear_t_prediction), accuracy_score(y_test, SVC_linear_t_prediction), \n",
    "                   roc_auc_score(y_test, SVC_linear_t_prediction)],\n",
    "             columns=['Tuned SVC RBF Kernel Score'],\n",
    "             index=[\"F1 Score\", \"Recall\", \"Precision\", \"Accuracy\", \"ROC AUC Score\"])\n",
    "df_SVC_linear_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtree = DecisionTreeClassifier(random_state=123)\n",
    "dtree.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# 예측 및 평가\n",
    "dtree_prediction = dtree.predict(X_test_scaled)\n",
    "cm_dtree = confusion_matrix(y_test, dtree_prediction)\n",
    "df_dtree = pd.DataFrame(data=[f1_score(y_test, dtree_prediction), recall_score(y_test, dtree_prediction),\n",
    "                   precision_score(y_test, dtree_prediction), accuracy_score(y_test, dtree_prediction), \n",
    "                   roc_auc_score(y_test, dtree_prediction)],\n",
    "             columns=['Decision Tree Score'],\n",
    "             index=[\"F1 Score\", \"Recall\", \"Precision\", \"Accuracy\", \"ROC AUC Score\"])\n",
    "df_dtree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_grid = {\n",
    "    \"max_depth\": [None, 10, 20, 30, 40], # None\n",
    "    \"min_samples_split\": [2, 10, 20, 50], # 2\n",
    "    # \"min_samples_leaf\": [1, 2, 5, 10], # 1\n",
    "    # \"criterion\": ['gini', 'entropy', 'log_loss'] # gini\n",
    "}\n",
    "\n",
    "# Initialize the GridSearchCV\n",
    "dtree_t = GridSearchCV(dtree, params_grid, cv=5, scoring='f1')\n",
    "dtree_t.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Output the best parameters\n",
    "print(\"Best Parameters: \", dtree_t.best_params_)\n",
    "\n",
    "# Predictions and evaluation\n",
    "dtree_t_prediction = dtree_t.predict(X_test_scaled)\n",
    "cm_dtree_t = confusion_matrix(y_test, dtree_t_prediction)\n",
    "df_dtree_t = pd.DataFrame(data=[f1_score(y_test, dtree_t_prediction), recall_score(y_test, dtree_t_prediction),\n",
    "                   precision_score(y_test, dtree_t_prediction), accuracy_score(y_test, dtree_t_prediction), \n",
    "                   roc_auc_score(y_test, dtree_t_prediction)],\n",
    "             columns=['Tuned Decision Tree Score'],\n",
    "             index=[\"F1 Score\", \"Recall\", \"Precision\", \"Accuracy\", \"ROC AUC Score\"])\n",
    "df_dtree_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier()\n",
    "rfc.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# 예측 및 평가\n",
    "rfc_prediction = rfc.predict(X_test_scaled)\n",
    "cm_rfc = confusion_matrix(y_test, rfc_prediction)\n",
    "df_rfc = pd.DataFrame(data=[f1_score(y_test, rfc_prediction), recall_score(y_test, rfc_prediction),\n",
    "                   precision_score(y_test, rfc_prediction), accuracy_score(y_test, rfc_prediction), \n",
    "                   roc_auc_score(y_test, rfc_prediction)],\n",
    "             columns=['Random Forest Score'],\n",
    "             index=[\"F1 Score\", \"Recall\", \"Precision\", \"Accuracy\", \"ROC AUC Score\"])\n",
    "df_dtree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_grid = {\n",
    "    'n_estimators': [100, 200, 300, 400, 500, 600], # 100\n",
    "    'max_depth': [10, 20, 30, 40, 50, None], # None\n",
    "    'min_samples_split': [2, 5, 10, 20], # 2\n",
    "    'max_features': ['sqrt', 'log2', None], # auto\n",
    "}\n",
    "\n",
    "# Initialize the GridSearchCV\n",
    "rfc_t = GridSearchCV(rfc, params_grid, cv=5, scoring='f1')\n",
    "rfc_t.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Output the best parameters\n",
    "print(\"Best Parameters: \", rfc_t.best_params_)\n",
    "\n",
    "# Predictions and evaluation\n",
    "rfc_t_prediction = rfc_t.predict(X_test_scaled)\n",
    "cm_rfc_t = confusion_matrix(y_test, rfc_t_prediction)\n",
    "df_rfc_t = pd.DataFrame(data=[f1_score(y_test, rfc_t_prediction), recall_score(y_test, rfc_t_prediction),\n",
    "                   precision_score(y_test, rfc_t_prediction), accuracy_score(y_test, rfc_t_prediction), \n",
    "                   roc_auc_score(y_test, rfc_t_prediction)],\n",
    "             columns=['Tuned Random Forest Score'],\n",
    "             index=[\"F1 Score\", \"Recall\", \"Precision\", \"Accuracy\", \"ROC AUC Score\"])\n",
    "df_rfc_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "random_state=0, n_estimators= 800, criterion = 'gini',max_features = 'sqrt',max_depth = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic = LogisticRegression()\n",
    "logistic.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# 예측 및 평가\n",
    "log_prediction = logistic.predict(X_test_scaled)\n",
    "cm_log = confusion_matrix(y_test, log_prediction)\n",
    "df_log = pd.DataFrame(data=[f1_score(y_test, log_prediction), recall_score(y_test, log_prediction),\n",
    "                   precision_score(y_test, log_prediction), accuracy_score(y_test, log_prediction), \n",
    "                   roc_auc_score(y_test, log_prediction)],\n",
    "             columns=['Logistic Regression Score'],\n",
    "             index=[\"F1 Score\", \"Recall\", \"Precision\", \"Accuracy\", \"ROC AUC Score\"])\n",
    "df_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100],  # 'C' 파라미터 값\n",
    "    'penalty': ['l1', 'l2', 'elasticnet'],  # 'penalty' 옵션\n",
    "    'solver': ['saga'],  # 모든 페널티 유형을 지원\n",
    "    'l1_ratio': [0, 0.5, 1]  # elasticnet 사용 시 필요한 l1_ratio 값\n",
    "}\n",
    "\n",
    "# Initialize the GridSearchCV\n",
    "logistic_t = GridSearchCV(logistic, params_grid, cv=5, scoring='f1')\n",
    "logistic_t.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Output the best parameters\n",
    "print(\"Best Parameters: \", logistic_t.best_params_)\n",
    "\n",
    "# Predictions and evaluation\n",
    "rfc_t_prediction = logistic_t.predict(X_test_scaled)\n",
    "cm_rfc_t = confusion_matrix(y_test, rfc_t_prediction)\n",
    "df_rfc_t = pd.DataFrame(data=[f1_score(y_test, rfc_t_prediction), recall_score(y_test, rfc_t_prediction),\n",
    "                   precision_score(y_test, rfc_t_prediction), accuracy_score(y_test, rfc_t_prediction), \n",
    "                   roc_auc_score(y_test, rfc_t_prediction)],\n",
    "             columns=['Tuned Random Forest Score'],\n",
    "             index=[\"F1 Score\", \"Recall\", \"Precision\", \"Accuracy\", \"ROC AUC Score\"])\n",
    "df_rfc_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X_train,y_train)\n",
    "\n",
    "knn_prediction = knn.predict(X_test_scaled)\n",
    "cm_knn = confusion_matrix(y_test, knn_prediction)\n",
    "df_knn = pd.DataFrame(data=[f1_score(y_test, knn_prediction), recall_score(y_test, knn_prediction),\n",
    "                   precision_score(y_test, knn_prediction), accuracy_score(y_test, knn_prediction), \n",
    "                   roc_auc_score(y_test, knn_prediction)],\n",
    "             columns=['KNN Score'],\n",
    "             index=[\"F1 Score\", \"Recall\", \"Precision\", \"Accuracy\", \"ROC AUC Score\"])\n",
    "df_knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_grid = {\n",
    "    'n_neighbors': [3, 5, 7, 9, 11, 17], # 5\n",
    "    'weights': ['uniform', 'distance'], # uniform\n",
    "    # 'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'], # auto\n",
    "    # 'leaf_size': [10, 20, 30, 40, 50] # 30\n",
    "}\n",
    "\n",
    "# Initialize the GridSearchCV\n",
    "knn_t = GridSearchCV(knn, params_grid, cv=5, scoring='f1')\n",
    "knn_t.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Output the best parameters\n",
    "print(\"Best Parameters: \", knn_t.best_params_)\n",
    "\n",
    "# Predictions and evaluation\n",
    "knn_t_prediction = knn_t.predict(X_test_scaled)\n",
    "cm_knn_t = confusion_matrix(y_test, knn_t_prediction)\n",
    "df_knn_t = pd.DataFrame(data=[f1_score(y_test, knn_t_prediction), recall_score(y_test, knn_t_prediction),\n",
    "                   precision_score(y_test, knn_t_prediction), accuracy_score(y_test, knn_t_prediction), \n",
    "                   roc_auc_score(y_test, knn_t_prediction)],\n",
    "             columns=['Tuned KNN Score'],\n",
    "             index=[\"F1 Score\", \"Recall\", \"Precision\", \"Accuracy\", \"ROC AUC Score\"])\n",
    "df_knn_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBClassifier()\n",
    "xgb.fit(X_train,y_train)\n",
    "\n",
    "xgb_prediction = xgb.predict(X_test_scaled)\n",
    "cm_xgb = confusion_matrix(y_test, xgb_prediction)\n",
    "df_xgb = pd.DataFrame(data=[f1_score(y_test, xgb_prediction), recall_score(y_test, xgb_prediction),\n",
    "                   precision_score(y_test, xgb_prediction), accuracy_score(y_test, xgb_prediction), \n",
    "                   roc_auc_score(y_test, xgb_prediction)],\n",
    "             columns=['XGBoost Score'],\n",
    "             index=[\"F1 Score\", \"Recall\", \"Precision\", \"Accuracy\", \"ROC AUC Score\"])\n",
    "df_xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_grid = {\n",
    "    'n_estimators': [100, 200, 300, 400, 500], # 100\n",
    "    'max_depth': [3, 5, 6, 7, 9, 11], # 6\n",
    "    # 'learning_rate': [0.01, 0.05, 0.1, 0.2, 0.3], # 0.3\n",
    "    # 'subsample': [0.6, 0.7, 0.8, 0.9, 1.0] # 1.0\n",
    "}\n",
    "\n",
    "# Initialize the GridSearchCV\n",
    "xgb_t = GridSearchCV(xgb, params_grid, cv=5, scoring='f1')\n",
    "xgb_t.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Output the best parameters\n",
    "print(\"Best Parameters: \", xgb_t.best_params_)\n",
    "\n",
    "# Predictions and evaluation\n",
    "xgb_t_prediction = xgb_t.predict(X_test_scaled)\n",
    "cm_xgb_t = confusion_matrix(y_test, xgb_t_prediction)\n",
    "df_xgb_t = pd.DataFrame(data=[f1_score(y_test, xgb_t_prediction), recall_score(y_test, xgb_t_prediction),\n",
    "                   precision_score(y_test, xgb_t_prediction), accuracy_score(y_test, xgb_t_prediction), \n",
    "                   roc_auc_score(y_test, xgb_t_prediction)],\n",
    "             columns=['Tuned XGBoost Score'],\n",
    "             index=[\"F1 Score\", \"Recall\", \"Precision\", \"Accuracy\", \"ROC AUC Score\"])\n",
    "df_xgb_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adab = AdaBoostClassifier()\n",
    "adab.fit(X_train,y_train)\n",
    "\n",
    "adab_prediction = adab.predict(X_test_scaled)\n",
    "cm_adab = confusion_matrix(y_test, adab_prediction)\n",
    "df_adab = pd.DataFrame(data=[f1_score(y_test, adab_prediction), recall_score(y_test, adab_prediction),\n",
    "                   precision_score(y_test, adab_prediction), accuracy_score(y_test, adab_prediction), \n",
    "                   roc_auc_score(y_test, adab_prediction)],\n",
    "             columns=['AdaBoost Score'],\n",
    "             index=[\"F1 Score\", \"Recall\", \"Precision\", \"Accuracy\", \"ROC AUC Score\"])\n",
    "df_adab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_grid = {\n",
    "    'n_estimators': [50, 100, 200, 300, 400, 500], # 50\n",
    "    'learning_rate': [0.01, 0.1, 0.5, 1.0], # 1.0\n",
    "    # 'algorithm': ['SAMME', 'SAMME.R'], # SAMME.R\n",
    "    # 'base_estimator': [DecisionTreeClassifier(max_depth=1), DecisionTreeClassifier(max_depth=2)] # DecisionTreeClassifier(max_depth=1)\n",
    "}\n",
    "\n",
    "# Initialize the GridSearchCV\n",
    "adab_t = GridSearchCV(adab, params_grid, cv=5, scoring='f1')\n",
    "adab_t.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Output the best parameters\n",
    "print(\"Best Parameters: \", adab_t.best_params_)\n",
    "\n",
    "# Predictions and evaluation\n",
    "xgb_t_prediction = adab_t.predict(X_test_scaled)\n",
    "cm_xgb_t = confusion_matrix(y_test, xgb_t_prediction)\n",
    "df_xgb_t = pd.DataFrame(data=[f1_score(y_test, xgb_t_prediction), recall_score(y_test, xgb_t_prediction),\n",
    "                   precision_score(y_test, xgb_t_prediction), accuracy_score(y_test, xgb_t_prediction), \n",
    "                   roc_auc_score(y_test, xgb_t_prediction)],\n",
    "             columns=['Tuned AdaBoost Score'],\n",
    "             index=[\"F1 Score\", \"Recall\", \"Precision\", \"Accuracy\", \"ROC AUC Score\"])\n",
    "df_xgb_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = GaussianNB()\n",
    "nb.fit(X_train,y_train)\n",
    "\n",
    "nb_prediction = nb.predict(X_test_scaled)\n",
    "cm_nb = confusion_matrix(y_test, nb_prediction)\n",
    "df_nb = pd.DataFrame(data=[f1_score(y_test, nb_prediction), recall_score(y_test, nb_prediction),\n",
    "                   precision_score(y_test, nb_prediction), accuracy_score(y_test, nb_prediction), \n",
    "                   roc_auc_score(y_test, nb_prediction)],\n",
    "             columns=['Naive Bayes Score'],\n",
    "             index=[\"F1 Score\", \"Recall\", \"Precision\", \"Accuracy\", \"ROC AUC Score\"])\n",
    "df_nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_grid = {\n",
    "    'var_smoothing': [1e-9, 1e-8, 1e-7, 1e-6], # 1e-9\n",
    "}\n",
    "\n",
    "# Initialize the GridSearchCV\n",
    "nb_t = GridSearchCV(nb, params_grid, cv=5, scoring='f1')\n",
    "nb_t.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Output the best parameters\n",
    "print(\"Best Parameters: \", nb_t.best_params_)\n",
    "\n",
    "# Predictions and evaluation\n",
    "nb_t_prediction = nb_t.predict(X_test_scaled)\n",
    "cm_nb_t = confusion_matrix(y_test, nb_t_prediction)\n",
    "df_nb_t = pd.DataFrame(data=[f1_score(y_test, nb_t_prediction), recall_score(y_test, nb_t_prediction),\n",
    "                   precision_score(y_test, nb_t_prediction), accuracy_score(y_test, nb_t_prediction), \n",
    "                   roc_auc_score(y_test, nb_t_prediction)],\n",
    "             columns=['Tuned Naive Bayes Score'],\n",
    "             index=[\"F1 Score\", \"Recall\", \"Precision\", \"Accuracy\", \"ROC AUC Score\"])\n",
    "df_nb_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume 'target' is the target column and the rest are features\n",
    "X_new_test = new_data_df.drop(columns=['target'])\n",
    "y_new_test = new_data_df['target']\n",
    "\n",
    "# Convert to numpy arrays for consistency\n",
    "X_new_test_array = X_new_test.values\n",
    "\n",
    "# 전처리 (Scaling) 적용\n",
    "scaler = StandardScaler()\n",
    "X_new_test_scaled = scaler.fit_transform(X_new_test_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_data(model, model_t):\n",
    "    prediction = model.predict(X_new_test_scaled)\n",
    "    prediction_t = model_t.predict(X_new_test_scaled)\n",
    "\n",
    "    cm_new = confusion_matrix(y_new_test, prediction)\n",
    "    df_new = pd.DataFrame(\n",
    "        data=[\n",
    "            f1_score(y_new_test, prediction),\n",
    "            recall_score(y_new_test, prediction),\n",
    "            precision_score(y_new_test, prediction),\n",
    "            accuracy_score(y_new_test, prediction),\n",
    "            roc_auc_score(y_new_test, prediction)\n",
    "        ],\n",
    "        columns=[f'{model} Score'],\n",
    "        index=[\"F1 Score\", \"Recall\", \"Precision\", \"Accuracy\", \"ROC AUC Score\"]\n",
    "    )\n",
    "\n",
    "    cm_new_t = confusion_matrix(y_new_test, prediction_t)\n",
    "    df_new_t = pd.DataFrame(\n",
    "        data=[\n",
    "            f1_score(y_new_test, prediction_t),\n",
    "            recall_score(y_new_test, prediction_t),\n",
    "            precision_score(y_new_test, prediction_t),\n",
    "            accuracy_score(y_new_test, prediction_t),\n",
    "            roc_auc_score(y_new_test, prediction_t)\n",
    "        ],\n",
    "        columns=[f'Tuned {model_t} Score'],\n",
    "        index=[\"F1 Score\", \"Recall\", \"Precision\", \"Accuracy\", \"ROC AUC Score\"]\n",
    "    )\n",
    "\n",
    "    # 결과 출력\n",
    "    print(\"Model result\")\n",
    "    print(cm_new)\n",
    "    print(df_new)\n",
    "\n",
    "    # 결과 출력\n",
    "    print(\"Tuned Model result\")\n",
    "    print(cm_new_t)\n",
    "    print(df_new_t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data(logistic, logistic_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "linearSVC, SVC_sigmoid, SVC_poly, SVC_rbf, SVC_linear, dtree, rfc, logistic, knn, xgb, adab, nb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# So far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,18), dpi=150)\n",
    "gs = fig.add_gridspec(4, 2)\n",
    "gs.update(wspace=0.1, hspace=0.5)\n",
    "ax0 = fig.add_subplot(gs[0, :])\n",
    "\n",
    "colors = [\"lightgray\",\"lightgray\"]\n",
    "colormap = matplotlib.colors.LinearSegmentedColormap.from_list(\"\", colors)\n",
    "# Change background color\n",
    "background_color = \"#fbfbfb\"\n",
    "fig.patch.set_facecolor(background_color) # figure background color\n",
    "ax0.set_facecolor(background_color)\n",
    "\n",
    "# Overall\n",
    "df_models = round(pd.concat([df_linearSVC, df_SVC_sigmoid, df_SVC_linear, df_SVC_RBF, df_SVC_poly,\n",
    "                             df_dtree, df_rfc, df_knn, df_xgb, df_adab, df_nb], axis=1),3)\n",
    "sns.heatmap(df_models.T, cmap=colormap,annot=True,fmt=\".1%\", linewidths=2.5,cbar=False,ax=ax0)\n",
    "\n",
    "ax0.tick_params(axis=u'both', which=u'both',length=0)\n",
    "ax0.text(0,-2,'Our results so far',fontfamily='serif',fontsize=20,fontweight='bold')\n",
    "\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "rect = ax0.add_patch(Rectangle((0, 0), 5, 1, fill=True,color='#0e4f66', edgecolor='white', lw=0,alpha=0.5))\n",
    "rect = ax0.add_patch(Rectangle((0, 2), 5, 1, fill=True,color='#0e4f66', edgecolor='white', lw=0,alpha=0.5))\n",
    "\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
